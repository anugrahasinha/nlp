{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wV3inYvPt9Z7"
   },
   "source": [
    "---------------------------------------------------------------\n",
    "***Author*** : Anugraha Sinha\n",
    "\n",
    "***Email*** : anugraha.sinha@gmail.com\n",
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwGDyr8Tt9Z9"
   },
   "source": [
    "## POS tagging using modified Viterbi Heuristic Based on Hidden Markov Model (HMM)\n",
    "**Objectives**\n",
    "\n",
    "We will be implementing POS Tagging prediction setup here using HMM and Viterbi Heuristic.\n",
    "While simple HMM based POS tagger has fairly good accuracy, but it is strongly dependent on the ***vocabulory*** on which it is built. That is, the prediction of POS Tags for those words, which are unknown/not present in the vocabulary becomes random, or to say the first element in the list of unique POS tags.\n",
    "\n",
    "The problem arises because of the formulation of HMM for this problem. The HMM based POS Tagger works on **emission probabilities** and **state transition probabilities**. When a word not present in the vocabulary is encountered, emission probability ( P(Word | Tag) = P(Word and Tag) / P(Tag) ) would become zero.\n",
    "\n",
    "This leads to net probabilitity calculation as zero, and hence the first tag in the list of unique POS Tag being chosen.\n",
    "\n",
    "We will aim to address this problem using 2 solutions\n",
    "\n",
    "1. We will use Lexical + N-Gram based Tagger. \n",
    "    * Lexical based taggers will try and address some specific patterns which are missed in prediction\n",
    "    * N-Gram Based taggers will use **Snowball Stemmer** and Unigram & Bigram tagger to for POS Tagging.\n",
    "2. When a word not present in vocabulary is found, we will only state-transition probability to calculate the POS Tag.\n",
    "\n",
    "The above strategies will be evaluated individually.\n",
    "\n",
    "**Wordbase used** : Python NLTK Library based **Universal Treebank Tag Set for English Language**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RoyfHKvt9Z_"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uf7z4Z17t9aA"
   },
   "source": [
    "Sourcing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:29:34.446161Z",
     "start_time": "2019-03-19T01:28:52.210587Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HzlJXew5t9aC"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uV-Dw9dtt9aH"
   },
   "source": [
    "Reading base data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:29:43.864005Z",
     "start_time": "2019-03-19T01:29:34.450149Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qGT9a-tKt9aJ"
   },
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzL1TKOZt9aM"
   },
   "source": [
    "Performing initial evaluation of the length/breath of the base data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:29:43.884976Z",
     "start_time": "2019-03-19T01:29:43.865961Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "juXvPdryt9aN",
    "outputId": "abb80db6-062f-4491-a2ad-017622754cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of sentences = 3914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('61', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('will', 'VERB'),\n",
       "  ('join', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('Nov.', 'NOUN'),\n",
       "  ('29', 'NUM'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Elsevier', 'NOUN'),\n",
       "  ('N.V.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Dutch', 'NOUN'),\n",
       "  ('publishing', 'VERB'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the data for once\n",
    "print(\"Total Number of sentences = %d\" %(len(nltk_data)))\n",
    "nltk_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UmkP41Kxt9aS"
   },
   "source": [
    "#### Create a training/test set \n",
    "* Lets build a train/test set of the base data.\n",
    "* We will be using 95/5 ratio for train/test, as HMM models can take a long time to predict, so to limit testing time, we take only 5% of data as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:29:43.998057Z",
     "start_time": "2019-03-19T01:29:43.892982Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hh5R08-et9aT",
    "outputId": "321a9df6-846f-41f9-90e9-3c0a1d6f275e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train data = 3718\n",
      "Number of sentences in test data = 196\n"
     ]
    }
   ],
   "source": [
    "# lets build a train and test set #\n",
    "train_data, test_data = train_test_split(nltk_data,test_size = 0.05,random_state=100)\n",
    "print(\"Number of sentences in train data = %d\" %(len(train_data)))\n",
    "print(\"Number of sentences in test data = %d\" %(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXjCnekPt9aY"
   },
   "source": [
    "#### **build_database** function\n",
    "Creates a composite database which has following important elements as a dictionary\n",
    "```\n",
    "1. base_data = original list of list structure of train data\n",
    "2. all_word_tag_incl_dup = all (word,tag) tuples in a single list (including duplicates)\n",
    "3. all_words_incl_dup = all words in a single list (including duplicates)\n",
    "4. all_tags_incl_dup = all tags in a single list (including duplicates)\n",
    "5. stemmed_base_data = (stemmed_word,tag) tuple list for all (word,tag) tuples in the train set\n",
    "6. vocab = all unique words \n",
    "7. unique_tags = all unique tags\n",
    "```\n",
    "The above data is structured in a python dict() and returned by the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:29:44.176186Z",
     "start_time": "2019-03-19T01:29:44.002060Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "z6_TQTDut9aZ"
   },
   "outputs": [],
   "source": [
    "# lets create a database of different selections from these sentences #\n",
    "# IMP : since we are iterating over the complete train corpus again again, it is better that we create nested loop\n",
    "#       here, rather than having a list comprehension for individual elements\n",
    "def build_database(data=train_data):\n",
    "    all_word_tag_incl_dup = list()\n",
    "    all_words_incl_dup = list()\n",
    "    all_tags_incl_dup = list()\n",
    "    vocab = list()\n",
    "    unique_tags = list()\n",
    "    stemmed_base_data = data\n",
    "    stem_obj = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    \n",
    "    for sent_key,sent in enumerate(data):\n",
    "        for word_tag_key,word_tag_tup in enumerate(sent):\n",
    "            # 1. All (word,tag) combinations - including duplicates - remember order is very important, dont change the order #\n",
    "            all_word_tag_incl_dup.append(word_tag_tup)\n",
    "            # 2. All words (only words) - including duplicates #\n",
    "            all_words_incl_dup.append(word_tag_tup[0])\n",
    "            # 3. All tags (only POS tags) - including duplicates - remember order is very important, dont change order #\n",
    "            all_tags_incl_dup.append(word_tag_tup[1])\n",
    "            # 4. formulate a stemmed version of the word #\n",
    "            stemmed_base_data[sent_key][word_tag_key] = (stem_obj.stem(word_tag_tup[0]),word_tag_tup[1])\n",
    "    vocab = list(set(all_words_incl_dup))\n",
    "    unique_tags = sorted(list(set(all_tags_incl_dup)),reverse=True)\n",
    "    # Important point to notice here #\n",
    "    # We should have the 1st tag in unique_tags as X, so that in vanilla viterbi, if a word is not found in vocab\n",
    "    # it get tags as X rather than a random selection TAG every time.\n",
    "\n",
    "    print(\"Unique words in train data set = %d\" %(len(vocab)))\n",
    "    print(\"Unique tags in train data set = %d\" %(len(unique_tags)))\n",
    "    print(\"Unique tags=\\n%s\" %(unique_tags))\n",
    "    db = {\"base_data\" : train_data,\n",
    "          \"stemmed_base_data\" : stemmed_base_data,\n",
    "          \"all_word_tag_incl_dup\" : all_word_tag_incl_dup,\n",
    "          \"all_words_incl_dup\" : all_words_incl_dup,\n",
    "          \"all_tags_incl_dup\" : all_tags_incl_dup,\n",
    "          \"vocab\" : vocab,\n",
    "          \"unique_tags\" : unique_tags}\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OorK8WjOt9ab"
   },
   "source": [
    "Building a composite database using build_database function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:29:46.104583Z",
     "start_time": "2019-03-19T01:29:44.179188Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "P7Kkp3WOt9ac",
    "outputId": "c2a74c76-ac2e-456c-9e37-1bee6e39c642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train data set = 12106\n",
      "Unique tags in train data set = 12\n",
      "Unique tags=\n",
      "['X', 'VERB', 'PRT', 'PRON', 'NUM', 'NOUN', 'DET', 'CONJ', 'ADV', 'ADP', 'ADJ', '.']\n"
     ]
    }
   ],
   "source": [
    "database = build_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T01:32:31.862514Z",
     "start_time": "2019-03-19T01:32:31.854508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 'NUM'),\n",
       " ('bright', 'ADJ'),\n",
       " ('sign', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('grow', 'VERB'),\n",
       " ('number', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('women', 'NOUN'),\n",
       " ('have', 'VERB'),\n",
       " ('enter', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('onc', 'ADV'),\n",
       " ('male-domin', 'ADJ'),\n",
       " ('field', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('more', 'ADJ'),\n",
       " ('than', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('third', 'ADJ'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('ringer', 'NOUN'),\n",
       " ('today', 'NOUN'),\n",
       " ('are', 'VERB'),\n",
       " ('women', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82qL-RSJt9af"
   },
   "source": [
    "#### calculate_prob_word_given_tag function\n",
    "```\n",
    "Builds the emission probability for a word, given POS Tag\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:19.909119Z",
     "start_time": "2018-10-07T06:38:19.903114Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QGsxVCgJt9ag"
   },
   "outputs": [],
   "source": [
    "# Calculate Emission Probabilities #\n",
    "def calculate_prob_word_given_tag(word,tag,data=database):\n",
    "    # P(W|T) = count(W and T) / (count(T))\n",
    "    count_word_and_tag = len(list(filter(lambda x : x[0] == word and x[1] == tag,data[\"all_word_tag_incl_dup\"])))\n",
    "    count_times_tag = len(list(filter(lambda x : x == tag,data[\"all_tags_incl_dup\"])))\n",
    "    return count_word_and_tag/count_times_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:22.106559Z",
     "start_time": "2018-10-07T06:38:22.056515Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4wPiSDLQt9aj",
    "outputId": "cb69df06-1272-44b5-b163-eb686a51a214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.631213914811722e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_prob_word_given_tag(\"Pierre\",\"NOUN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5toW0Y9Tt9ao"
   },
   "source": [
    "#### calculate_transition_prob function\n",
    "```\n",
    "Calculated the transition probability from state (POST TAG t1) -> state (POS TAG t2) in an HMM structure.\n",
    "function signature expects tags as follows\n",
    "calculate_transition_prob(t2,t1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:23.586513Z",
     "start_time": "2018-10-07T06:38:23.580507Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cQOHhL_nt9ap"
   },
   "outputs": [],
   "source": [
    "# Calculate Transition Probabilities #\n",
    "def calculate_transition_prob(t2,t1,data=database):\n",
    "    # P(T2/T1) = count(T2 ahead of T1)  /  count(T1)\n",
    "    count_t2_ahead_t1 = len(list(filter(lambda x : data[\"all_tags_incl_dup\"][x] == t1 and data[\"all_tags_incl_dup\"][x+1] == t2,np.arange(0,len(data[\"all_tags_incl_dup\"])-1))))\n",
    "    count_t1 = len(list(filter(lambda x : x == t1,data[\"all_tags_incl_dup\"])))\n",
    "    return count_t2_ahead_t1 / count_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:26.065414Z",
     "start_time": "2018-10-07T06:38:25.998383Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K6GpcCivt9ar",
    "outputId": "ee6a7a69-a26d-4aa3-fca7-9b1a99ed5f38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7009009009009008"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_transition_prob(\"NOUN\",\"ADJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XktcRm_Ct9av"
   },
   "source": [
    "#### state (tag) transition matrix\n",
    "Lets create a state transition matrix. There are 12 unique tags in the dataset, so we can build 12X12 state transition matrix signifying the probability of transition from 1 tag to another tag\n",
    "\n",
    "**Important**\n",
    "\n",
    "In this state transition matrix\n",
    "```\n",
    "rows    = T2 Tag\n",
    "columns = T1 Tag\n",
    "```\n",
    "\n",
    "We will save this state transisition matrix in the main database python dict() also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:35.291409Z",
     "start_time": "2018-10-07T06:38:29.472331Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jZeJPm-2t9aw"
   },
   "outputs": [],
   "source": [
    "# Lets create tag to tag matrix, having t2 and t1 given #\n",
    "# Rows = T2 and columns = T1\n",
    "tag_transition_df = pd.DataFrame(np.array([calculate_transition_prob(t2,t1) for t2 in database[\"unique_tags\"] for t1 in database[\"unique_tags\"]]).reshape(len(database[\"unique_tags\"]),len(database[\"unique_tags\"])),\n",
    "                                 columns=database[\"unique_tags\"],index=database[\"unique_tags\"])\n",
    "database[\"tag_transition_df\"] = tag_transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:38.130569Z",
     "start_time": "2018-10-07T06:38:38.027752Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "czG5KFkht9ay",
    "outputId": "24f454c9-191c-4d31-e40a-238dbc71c0af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.074433</td>\n",
       "      <td>0.218438</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.092720</td>\n",
       "      <td>0.211824</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>0.045323</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.023302</td>\n",
       "      <td>0.034984</td>\n",
       "      <td>0.020311</td>\n",
       "      <td>0.026908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.204571</td>\n",
       "      <td>0.168744</td>\n",
       "      <td>0.405184</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.146955</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>0.344541</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.088708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.184891</td>\n",
       "      <td>0.031427</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.043357</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>0.002511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.055705</td>\n",
       "      <td>0.035916</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.058414</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.069119</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.065208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.022448</td>\n",
       "      <td>0.056102</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.184195</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.021640</td>\n",
       "      <td>0.042188</td>\n",
       "      <td>0.031625</td>\n",
       "      <td>0.061910</td>\n",
       "      <td>0.020803</td>\n",
       "      <td>0.081353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.062371</td>\n",
       "      <td>0.111386</td>\n",
       "      <td>0.245735</td>\n",
       "      <td>0.211494</td>\n",
       "      <td>0.352347</td>\n",
       "      <td>0.264280</td>\n",
       "      <td>0.637293</td>\n",
       "      <td>0.350487</td>\n",
       "      <td>0.031625</td>\n",
       "      <td>0.321213</td>\n",
       "      <td>0.700901</td>\n",
       "      <td>0.222531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.055229</td>\n",
       "      <td>0.133292</td>\n",
       "      <td>0.101050</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.118683</td>\n",
       "      <td>0.069907</td>\n",
       "      <td>0.323969</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.173558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.010316</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.042921</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.058032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.082050</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>0.053778</td>\n",
       "      <td>0.077230</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.052292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.144898</td>\n",
       "      <td>0.091184</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.035056</td>\n",
       "      <td>0.177058</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.053778</td>\n",
       "      <td>0.119507</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.078624</td>\n",
       "      <td>0.092206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.083661</td>\n",
       "      <td>0.072031</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>0.204977</td>\n",
       "      <td>0.118683</td>\n",
       "      <td>0.130160</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.067158</td>\n",
       "      <td>0.043681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.162831</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>0.118835</td>\n",
       "      <td>0.239951</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.135153</td>\n",
       "      <td>0.039754</td>\n",
       "      <td>0.063882</td>\n",
       "      <td>0.092923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             X      VERB       PRT      PRON       NUM      NOUN       DET  \\\n",
       "X     0.074433  0.218438  0.013123  0.092720  0.211824  0.028868  0.045323   \n",
       "VERB  0.204571  0.168744  0.405184  0.484291  0.016934  0.146955  0.040394   \n",
       "PRT   0.184891  0.031427  0.001969  0.012261  0.026144  0.043357  0.000240   \n",
       "PRON  0.055705  0.035916  0.017717  0.007663  0.001485  0.004721  0.003727   \n",
       "NUM   0.002857  0.022448  0.056102  0.007280  0.184195  0.009550  0.021640   \n",
       "NOUN  0.062371  0.111386  0.245735  0.211494  0.352347  0.264280  0.637293   \n",
       "DET   0.055229  0.133292  0.101050  0.009195  0.003862  0.013363  0.005771   \n",
       "CONJ  0.010316  0.005186  0.002297  0.004981  0.013072  0.042921  0.000481   \n",
       "ADV   0.025393  0.082050  0.010171  0.034100  0.002674  0.016813  0.012623   \n",
       "ADP   0.144898  0.091184  0.019357  0.023372  0.035056  0.177058  0.009618   \n",
       "ADJ   0.016505  0.065640  0.083661  0.072031  0.033571  0.012165  0.204977   \n",
       ".     0.162831  0.034291  0.043635  0.040613  0.118835  0.239951  0.017913   \n",
       "\n",
       "          CONJ       ADV       ADP       ADJ         .  \n",
       "X     0.008809  0.023302  0.034984  0.020311  0.026908  \n",
       "VERB  0.155308  0.344541  0.008481  0.011794  0.088708  \n",
       "PRT   0.003709  0.014314  0.001484  0.010156  0.002511  \n",
       "PRON  0.058414  0.015646  0.069119  0.000491  0.065208  \n",
       "NUM   0.042188  0.031625  0.061910  0.020803  0.081353  \n",
       "NOUN  0.350487  0.031625  0.321213  0.700901  0.222531  \n",
       "DET   0.118683  0.069907  0.323969  0.004914  0.173558  \n",
       "CONJ  0.000464  0.006991  0.000848  0.016052  0.058032  \n",
       "ADV   0.053778  0.077230  0.013357  0.004914  0.052292  \n",
       "ADP   0.053778  0.119507  0.017492  0.078624  0.092206  \n",
       "ADJ   0.118683  0.130160  0.107389  0.067158  0.043681  \n",
       ".     0.035698  0.135153  0.039754  0.063882  0.092923  "
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_transition_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrepGSPJt9a5"
   },
   "source": [
    "Lets see a heatmap of the state transistion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:40.911437Z",
     "start_time": "2018-10-07T06:38:40.432810Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZvT_NO-Ot9a8",
    "outputId": "f45158fb-2d20-433e-f6ca-0d081727e28b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7636dc1898>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEQCAYAAAC6Om+RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmcXXV9//HXO5OEBEKAGBZZQ1gEwi4iP5YWBRRbFbEiYK3SHzX6K4jayg9EXIBWqYq2KC5pS1ErW0vFlFLEFuGHC2UNQoJIEhDCIhAQEhNIZubz++Ocmzm53Llz557znbn35v3kcR6Zs33Od4bkM9/7Pd9FEYGZmfWeCeNdADMzS8MJ3sysRznBm5n1KCd4M7Me5QRvZtajnODNzHqUE7yZWY9ygjcz61FO8GZmPWrieBdgtPbc6uAkQ2/fufGuKcJy0VM/SRIXYK8tdkwS95iNdkgSd96zdySJCzBl4uQkcZ97aUWSuBv1TUoSF2ByX5p/1i/1r00St29CunrmCyuXqGyMtc8ubTnnTJo5u/TzquQavJlZj+q6GryZ2ZgaSPPJZSw4wZuZNTM4ON4laJsTvJlZExFO8GZmvamLa/Dj+pJV0g6SHpY0I9/fIt/faTzLZWa2Tgy2vnWYcU3wEfEY8A3gwvzQhcC8iPj1+JXKzKxgcKD1rcN0QhPNV4C7JH0UOBz48DiXx8xsyED/eJegbeOe4CNiraQzgRuAN0XEmvprJM0F5gJsM20nNp+61RiX0sw2VN38krVTBjq9BXgS2LvRyYiYFxEHRcRBTu5mNqYGB1vfOsy4J3hJ+wPHAIcAH5P06nEukpnZEL9kbY8kkb1k/WhEPAp8EfjSeJbJzGw9XfySdbxr8B8AHo2IH+X7Xwf2kPT741gmM7MhA/2tbx1mXF+yRsQ8YF5hfwB47fiVyMysTgc2vbRq3HvRmJl1tA58edoqJ3gzsyayhoXu5ARvZtaMm2jGzls2np0k7qPxUpK4D+2zW5K4AJu/fqMkcbe59OdJ4r5p5j5J4gL87IWHksR9+9YHJon7l2v7ksQF+P3n0qycJdIsVrTb9G2TxK2Mm2jMzHqUF/wwM+tRbqIxM+tRbqIxM+tRrsGbmfWoLq7Bj2qqAkk3S3pz3bGPSrpe0mpJCwrb+/Lzj0i6T9IvJN1SXK1J0kB+7b2S7pZ0aDXflplZRbp4NsnR1uCvAE4Cflg4dhJwJrBjROw/zH1viIhnJZ0HnEs2Bw3A6to9+S+OzwOeh8bMOkZ0cS+a0U429q/AWyVtBCBpFrAtsKzF+38ObDfMuenA86Msj5lZWhvKdMERsRy4HTg2P3QScBUQwC51TTRHNAhxLHBtYX9qfu0vgX8ALmj0XElzJd0p6c5frFgymiKbmZXTxU007UwXXGumIf/zivzrJRGxf2G7tXDPjyU9DRwNXF44vjq/dg+y5P+dfI749RRXdNp3013aKLKZWZsqrMFLOlbSg5IWSzp7mGveLWmRpIWSLi8cf7+kh/Lt/a0UvZ0Efy1wlKQDgakRcXcL97wB2AlYCJzf6IKI+DkwE9iyjTKZmaVRUQ1eUh9wCdkSpXsBJ0vaq+6a3YBPAIdFxBzgo/nxGcBngNcDBwOfkbTFSEUfdYKPiJXAzcClDNXeW7lvdV7Y9+WFXY+kPYA+YPloy2Rmlkx1C34cDCyOiKURsQa4Ejiu7poPAJdExPMAEfF0fvzNwI8i4rn83I8YaiofVrsrOl0B7JcXsKa+Df6M+psi4sn83tPyQ7U2+AVkbfnvj26em9PMek91bfDbAY8V9pfxyk4nuwO7S/qppNskHTuKe1+hrYFOEfF9GJpaLiIeAaYOc+2suv0PF75ON6WemVkVRtE7RtJcYG7h0Lx85Tqg4XScUbc/EdgNOBLYHrhV0t4t3vsKHslqZtbMKHrH1C9DWmcZsENhf3vgiQbX3BYRa4GHJT1IlvCXkSX94r03j1Se8V5028yss1XXi+YOYDdJO0uaTNYLcX7dNdeSdUpB0kyyJpulZINL3yRpi/zl6ptYf8BpQ67Bm5k1U1H/9ojol3Q6WWLuAy6NiIWSzgfujIj5DCXyRcAAcGY+/ghJF5D9kgA4PyKeG+mZihixGaejfHDWCUkK/C/LF6QIy4svr0oSF2DihDSvMDaaOClJ3KkTJyeJC/DsqheTxE2zhhFM7EtXt1o7cm+OtvRNSPOBfyDhAKH+NY+X/l+4+l//quWcM/Vd56b6K9MW1+DNzJrpwBGqrXKCNzNrpstaOYqc4M3MmnEN3sysRznBm5n1qA6cBrhVlSZ4SQPAfXncB8imHlhVd/xh4E/IOvx/N791R+CFfHs2Io6uslxmZm0b6N7ZU6ru91Sb/ndvYA3woQbHnwNOi4j7alMLk3X2PzPfd3I3s87RxfPBp2yiuRXYt8Hxnw9z3Mys83Rg4m5VkpELkiaSzXl8X93xPuAoXjk818ysM20oS/a1YGo+9e+dwKPAP9YdXw7MIJvLuGXFJfseWLG00gKbmTUTg9Hy1mlStcHvHxEfzie1X3ecbFWnyQzNB9+S4pJ9e246u+Iim5k1Ud2CH2NuTGeTjIgXgDOAj0tKM+GJmVmVBqP1rcOM+XTBEXEPcC9DC3ebmXUu96LJRMS0Vo5HxNvq9k+pshxmZpXpwMTdKo9kNTNrxpONmZn1KNfgzcx6VBdPVdB1Cf6GFQ8miZtqtZpUqyNBupV7frfmpSRxVyWKC+lWXtp+05lJ4j616rdJ4gLM3uzVSeIufeHJJHF3mr51kriV6cDeMa3qugRvZjaWwk00ZmY9yjV4M7Me1YFzzLTKCd7MrJl+v2Q1M+tNXdxE03LXEUkDkhZIul/Sv0jauMHxf5e0eeGeOZJukvQrSQ9J+pQk5edOkTQoad/C9fdLmlXdt2dmVtIGMl1wy6s1AUiaSjbv+4URsTuwH3Ao8OeFmMuAT5b8HszM0tkAJxu7Fdi1wfGfA9vlX78H+GlE3AgQEauA04GzC9dfB8yR9Jo2y2FmllQMDra8dZpRJ/hRrNY0B7ireE1ELAGmSZqeHxoEvgCcM9pymJmNiQ2kBj/a1ZoEDPcdF49fDhwiaefhHlxc0WnlS8+NoshmZiUNDLS+dZh22uBbXa1pIXBQMYCk2cDKiFhROxYR/cBFwFnDPbi4otO0KTNGUWQzs5I2kBp8Uw1Wa/oecLiko2HdS9eLyZpk6l0GHA1sWVV5zMyq4DVZc8XVmiJiNXAccK6kB8na7O8AvtbgvjVkyX+rKstjZlZaF9fgWx7o1M5qTRFxH3DkMPddRlZzr+1fTJbkzcw6Rwf2jmmVR7KamTXTgTXzVjnBm5k1EQOuwZuZ9SbX4MfO4yuWJ4m78aSNksSdOXX6yBe16cmVacYEnLrtoUniXvbUbUniAkyakOav8uqBNSNf1IapEycniQvQH2n6Y09JVOapfel+FpVwgjcz602d2P2xVU7wZmbNdHGCT7PStJlZj4j+aHkbiaRjJT0oabGks5tc9y5JIemgfH+WpNX51OwLJH2zlbK7Bm9m1kxFNfh8QsZLgGPIpkq/Q9L8iFhUd92mZLMC/E9diCX5tDAtcw3ezKyZwVFszR0MLI6Ipfno/SvJRvvXu4BsSpeXyha9VILPP0JcVNj/uKTP5l9fJuldddevzP+cld97QeHcTElrJb1iKgMzs/EymrloijPf5tvcQqjtgMcK+8sYWj8DAEkHADtExHUNirKzpHsk3SLpiFbKXraJ5mXgnZI+HxHPjvLepcBbgU/l+yeQzUBpZtY5RjHOKSLmAfOGOa1Gt6w7KU0AvgKc0uC6J4EdI2K5pNcC10qaExEvNitP2SaafrJv5mNt3LsaeKD2EgE4Ebi6ZHnMzCpV4WySy4AdCvvbA08U9jcF9gZulvQIcAgwX9JBEfFyRCwHiIi7gCXA7iM9sIo2+EuAP5a0WRv3XgmcJGl7YID1v1kzs3EX/a1vI7gD2E3SzpImAycxtAIeEfFCRMyMiFkRMQu4DXh7RNwpacv8JW1tXY3dyFpBmirdiyYiXpT0HbK3vquLpxpdXrd/A9kLhd8AVw33jLwday6A+jZjwoRNSpXZzKxlFU1FExH9kk4Hfgj0AZdGxEJJ5wN3RsT8Jrf/HnC+pH6yyvCHImLEoexVdZP8W+Bu4J8Kx5YDW9R2JM0A1munj4g1ku4C/pJsDde30UCxXWvi5O26d9SBmXWdqHCusYi4Hri+7tinh7n2yMLX1wDXjPZ5lXSTzH+TXA2cWjh8M3Bi/lEEshcHP25w+0XAWbX2JTOzjlJdN8kxV+VAp4uA02s7EXFd/rb3LkkDZC8FPlR/U0QsxL1nzKxDVVmDH2ulEnxxNaeI+A2wcd3584DzGtz3CNnb4vrjl1FY5cnMbLxtsAnezKzXxUCj7uvdwQnezKwJ1+DNzHpUDLoGP2Z2nL5VkrjbbTQjSdzj+l6dJC7AM5umWbnnhP7VI1/UhgO2/P0kcQFOuabRnE3l3Xf85UnivvnF+5PEBfhW34gDHNty5KOfTxL3goM+NfJF48g1eDOzHhXhGryZWU8a7HeCNzPrSdHFY+ed4M3MmvBLVjOzHtXNCX7EuWiardqU78+V9Mt8u13S4YVzj0iaWdg/UtJ1+denSBqUtG/h/P2SZpX+rszMKhLR+tZpWplsrLZq08z6E5LeCnwQODwi9iCba+ZySdu0+PxlwCdbLayZ2ViLQbW8dZpWEnyzVZvOAs6sLdcXEXcD3wZOa/H51wFzJL2mxevNzMbU4IBa3jpNq9MFD7dq0xzgrrpjd+bHWzFItnr4OS1eb2Y2pgZDLW+dpqUEny/sWlu1aSRiaOWmVlZ1uhw4RNLOwwYsrFT+4kujXdvbzKx9EWp56zSjWfDjb8kW9Ciul7cIeG3ddQfmx6FuVSeg0apO/eSLfgz34IiYFxEHRcRB06e84lWAmVkyvd4GDwy7atMXgL+R9CoASfuTrdz09fz8zcCf5Of6gPfSeFWny4CjgS1HU3gzs9R6vRdN0UXAuip0vkjspcDPJP0S+HvgvRHxZH7JBcCuku4F7gEWA/9cHzQi1gAXA2lmEjMza1M31+BHHOjUwqpN3wC+Mcy9LwDvGebcZRRWb4qIi8mSvJlZxxgYrGTp6nHhkaxmZk10YtNLq5zgzcya6MTuj61ygjcza6ITuz+2qusS/ECkWcVo2oTJSeL+dyxn+cCqJLHvfW5pkrhfHky3hE3fhL4kcc844uYkcQ/bcs8kcWdOqR8zWJ1ULcYbb3tEkrjv3faQJHGr4iYaG1aq5N6NUiV3s5T8ktXMrEe5Dd7MrEd1cQuNE7yZWTOuwZuZ9Sj3ogEkDQD3AZPI5pD/NvC3ETEo6UjgB8DDhVs+D3wi/3obYAB4Jt8/OJ++wMxsXKXrU5ZelTX41RGxP4CkrcimAd4M+Ex+/taIeGvdPVfl138WWBkRX6qwPGZmpQ10cQ0+Sf+fiHgamAucLql7fzpmtsEbRC1vnSZZG3xELJU0gaEZIo+QtKBwyR9FxJJUzzczq0J0YOJuVeqXrMWfTKMmmtaCSHPJPhEwY+PtmDZlRhVlMzMbUTe3wScboiVpNtmL06fLxiqu6OTkbmZjKVDLW6dJUoOXtCXwTeBrERFuhjezbtU/3gUoocoa/FRJCyQtBP4LuBE4r3D+iPx8bXtXhc82M0uiyhq8pGMlPShpsaSzG5z/kKT78hz5E0l7Fc59Ir/vQUlvbqXsldXgI2LYmaQi4mayLpPDnf9sVeUwM6tSVSvx5etSXwIcAywD7pA0PyIWFS67PCK+mV//duDLwLF5oj8JmANsC/yXpN0jmk+v273TpJmZjYEKu0keDCyOiKX5QM4rgeOKF0TEi4XdTRiaCuc44MqIeDkiHiZb3/rgkR7oqQrMzJqocLKx7YDHCvvLgNfXXyTpNOAvgMnAGwv33lZ373YjPdA1eDOzJgZHsUmaK+nOwja3EKpRFf8Vvz8i4pKI2AU4Czh3NPfWcw3ezKyJgVH0AoyIecC8YU4vA3Yo7G8PPNEk3JXAN9q8F+jCBP/U755PEveJFcuTxE05l3SqzqepVl7afMomSeICPLvqxZEvasOtTy8a+aI2TEjYdfjN8WSSuJP60qSLm1csThK3KhUOdLoD2E3SzsDjZC9N31O8QNJuEfFQvvuHQO3r+cDlkr5M9pJ1N+D2kR7YdQnezGwsVdWLJiL6JZ0O/BDoAy6NiIWSzgfujIj5ZPN3HQ2sBZ4H3p/fu1DS1cAisq75p43Ugwac4M3MmqpyErGIuB64vu7Ypwtff6TJvX8N/PVonucEb2bWhJfsMzPrUVU10YwHJ3gzsyZGbOjuYG33g5e0jaQrJS2RtEjS9ZJ2lzRH0k2SfiXpIUmfqi36IekUSYOS9i3EuV/SrPzrRyTNLPtNmZlVZVCtb52mrQSfJ+zvAzdHxC4RsRdwDrA1WXeeCyNid2A/4FDgzwu3LwM+WarUZmZjZDQDnTpNuzX4NwBra5PiAETEAmB34KcRcWN+bBVwOlCcNe06YI6k17T5bDOzMbMhJvi9gbsaHJ9Tfzxflm+apOn5oUHgC2Q1/pYUh/8ODKxss8hmZqMXan3rNFXPRSOG71VUPH45cEg+omtExRWd+vqmlS2jmVnL+kexdZp2E/xC4LXDHD+oeCBfum9lRKyoHYuIfuAissl0zMw6Voxi6zTtJvibgI0kfaB2QNLryOZNODwfaoukqcDFZE0y9S4Djga2bLMMZmbJbXC9aCIigOOBY/JukguBz5LNbnYccK6kB4H7yCbY+VqDGGvIkv9WhcMTgZfbKZOZWQrd/JK17YFOEfEE8O5hTh85zD2XkdXca/sXkyX52kLdKjblmJmNt05M3K3qiAU/8rUHbwU+Md5lMTMr6uY2+I6YqiCfJnP+eJfDzKxefwe2rbeqIxK8mVmn6sSaeau6LsFPmTg5SdwdNknTmeehFx5PEhdg08lTk8T97Uu/SxJ3xZrVSeICTEy0CtVWG2+WJO6mkzZOEhdgyQtpVnTqH0jT03sgOruVe7CLU3zXJXgzs7HU2b9+mnOCNzNronvr707wZmZNuQZvZtaj+tW9dXgneDOzJro3vVc40EnS8ZJC0h75/ixJqyXdI+kBSbdLen/h3DJJE+piLJB0cFVlMjMrq5unKqhyJOvJwE+AkwrHlkTEARGxZ378Y5L+NCIeAR4DjqhdmP9i2DQibq+wTGZmpQwSLW+dppIEL2kacBhwKusn+HUiYinwF8AZ+aEr6q49KT9mZtYxunmqgqpq8O8AboiIXwHPSTpwmOvuBvbIv74aeIek2nuAE4ErG91UXNFpzdoXKyqymdnI+omWt05TVYI/maHkfGW+38i6WR0i4imyBUKOkrQ/2Rqv9ze6qbii0+RJ0xtdYmaWRDfX4Ev3opH0KuCNwN6SAugj+16/3uDyA4AHCvu1Zprf4OYZM+tAnfjytFVVdJN8F/CdiPhg7YCkW4DtixdJmgV8Cfhq4fA1wOeAVWS/JMzMOkp0ZN28NVUk+JOBC+uOXQOcA+wi6R5gCrAC+GpE/FPtooj4raTbgK0j4uEKymJmVqkNugYfEUc2OLZupaYW7j+ubBnMzFLpxO6PrfJIVjOzJgac4M3MetMG3URjZtbLNvSXrGPqyBl7Jon72NrfJombaqUhgDWJVtjZfMomSeK+fca+SeICfO+p/0kWO4WtEo7neGTCb5LE3WWzVyeJu9GESUniVsU1eDOzHuUavJlZj3IN3sysRw2Ea/BmZj2pm/vBVzkfvJlZz4lR/DcSScdKelDSYklnNzj/e5LultQv6V115wbyRZEWSJrfStkrTfCjWdUpP3+KpGfyAi+S9IEqy2NmVlZVKzpJ6gMuAd4C7AWcLGmvusseBU4BLm8QYnVE7J9vb2+l7FU30RRXdfpsfmxJRBwAIGk28G+SJhTmpLkqIk6XtBWwUNL8iEjTz8vMbJQqbKI5GFicL36EpCuB44BFtQvy1e6QVMm73SrXZG1nVafiuaeBJcBOVZXJzKysAaLlrbg4Ub7NLYTajmyp0ppl+bFWTclj3ibpHa3cUGUNft2qTpJqqzo91+C64qpO6+S1+9nA4gbn5gJzAfbdYh9mTduxwmKbmQ0vRtGLJiLmAfOGOa0Gx0bz8WDHiHgiz5U3SbovIpY0u6HqRbdHtapT7kRJC8gW/PhgRLzil0JxRScndzMbSxUuur0M2KGwvz3wRKvliIgn8j+XAjeTLaDUVCU1+JKrOl0VEadXUQ4zs6pVONDpDmA3STsDj5M1Zb+nlRslbQGsioiXJc0kaw7/wkj3VVWDr63qtFNEzIqIHYCHaW1VJzOzjlVVN8mI6AdOB35IVsm9OiIWSjpf0tsBJL1O0jLgBOBbkhbmt+8J3CnpXuDHwIURseiVT1lfVW3wba/qZGbWyaoc6BQR1wPX1x37dOHrO6irGOfHfwbsM9rnVZLg213VKSIuAy6rogxmZil4qgIzsx7l2STNzHpUN89F4wRvZtbEaPrBd5quS/A3PnNfkrjbbvKqJHF3mZ5mFRyANYNpVnR69qUXksS9/Knbk8QFOG2bw5LEvad/eZK4k5Xun97+W8xOEvfpNWn+Xix/+cUkcaviGryZWY8aiO5d8sMJ3sysie6tvzvBm5k15SYaM7Me5QRvZtajurkXTbIl+9pc3elrqcpjZtaOCmeTHHMpa/DtrO5kZtZRBru4F02SGnzZ1Z3MzDpFN9fgUzXRrFvdCait7tRIw9Wd6hWXwervX1llOc3MmoqIlrdOkyrBt7u6U0PFFZ0mTpxWRfnMzFrSzTX4ytvgS67uZGbWUTyb5Ppqqzt9sHZA0i14dScz60KDHdj00qoUCb7d1Z0mAi8nKI+ZWds8F01Bu6s7AXOAh6ouj5lZGW6iKUnSfwKTGeovb2bWEdxEU1JEvGW8y2Bm1ohr8GZmPco1+DF0/FbDjZkq58bnFyaJ+7v+1UniQrpJkPaZvlOSuG/u2zpJXIALnv5JkrhTJ05OEvfgzXdNEhdg8conksSdNmnjJHG3nZpmNbWqDMbAeBehbV2X4M3MxlInDmBqlRO8mVkTnTgFQauc4M3MmnAN3sysR7kGb2bWo9yLxsysR3Xzgh9O8GZmTbgN3sysR7kN3sysR3VzG3yqFZ0qVVyyb/HKR8a7OGa2AfGSfYkVl+zbddqs8S6OmW1AvGSfmVmPGhjs3l40HVWDl3S9pG3HuxxmZjUxiv86TUfV4CPiD8a7DGZmRd38krWjEryZWafpxJenrXKCNzNrohObXlrlBG9m1sRgF79kdYI3M2uie+vvjK4Tf7dtwNxui91tcbuxzP5Z+GexoWwd1U0ygbldGLvb4qaM3W1xU8butrgpY6csc0/p9QRvZrbBcoI3M+tRvZ7g53Vh7G6LmzJ2t8VNGbvb4qaMnbLMPUX5SwszM+sxvV6DNzPbYDnBm5n1qJ5J8JJ2aHLuiLEsi3UOSR7MZxusnmmDl7QU+Cbw5Yjoz49tDVwEvCYiXlfhs2YCy6PkD0/SjRHxpoqKVYy7Y7PzEfFo1c8sK1WZJd0dEQe2V6qmcWc0Of1yRPyuzbgHRcSdbRZrpNhbAjsBiyPitymeMZYkbRMRT413OTpZL9VuXgtcCNwj6SPAPsBfAF8A3tduUEmH5HGfAy4AvgvMBCZIel9E3FCizFuWuLeZ/yAbYa3CsciftxXQ125gSRc3Ox8RZ7QZOlWZNfIlbbmLV5a3ZqIkgLMj4nujjPv3kqYBVwBXRsSicsXMSPoz4HPAEmBnSXMjYn5Fsb/K8CP6X86f+b2IWFHF8wr+EfjDimP2lJ6pwdfkyf0rwBPAIRGxrGS8O4FzgM3Iume9JSJuk7QHcEVEHFAi9lLg48Odj4h/azd23XNmAWcBRwMXR8RXS8RaA9wPXE32M14vwUXEt9su6PrPmUUFZZa0DPjycOcjYthzZeS15VsiYq827n0NcBJwIrCGoWT/6xLluR94Q0Q8I2k2WcL9X+3Gq4v9/ianJwJzgH0i4pgqnmet65kavKTNgb8BXg8cC/wB8J+SPhIRN5UIPTEibsyfcX5E3AYQEb/Ma2llbAa8lca1wABKJXhJuwGfJPuZXAScERFry8QEXg2cQJZ8+oGrgGsi4vmScYEkZe4DplFxTX6EJqWIiMckndVO7Ih4EDgPOE/SfmTJ/iZJT0XEYe3EBNZExDN5/KWSNmozziu08ktd0vVVPc9a1zMJHrgb+DpwWt4Gf6Ok/YGvS/p1RJzcZtziXKGr686V/fjz64j43yVjvIKkvcmS5ByyJqpTI2KgitgRsZzsXcc3JW0HnAwslHRWRHy3A8v8ZEScX0GceiM2KUXEv5d5gKQJeaytgU2AZ0qE276ueW29/RJNa8C6WvxHgNfkhx4g++T1nTy+V2sbB72U4H+vvjkmIhYAh0r6QIm4+0l6kewf8tT8a/L9KSXi1mKkcC/wGFkSOhg4uPhpo+w/ZgBJB5Il92OA/yRrky4jVZmT/IwjYp/1HrJ+k9LnysTOe32dDLyDrDnsSuBjEfFCibBn1u2X/f+1jqT3AR8le+d1N9nP/EDgi5KoJXkbez3XBt9NJO0dEfc3ON4HnNTGC7ra/c3aREu1k0s6j6xZ6QGyxHNDrddSGZJOocknonbLLOndEXF1/vXOEfFw4dw7y77naNCk9O0yTUqSHgMeJfvZXh0RvylTvrEg6Tayv6+P1B2fRfbu4JBxKJbhBN+2vM3/tIj46xIxpgOnAdsB84EfAaeTvXhdEBHHVVDOaWRtwm1122sQbxBYylBzVe0vkPLn7FvFc6pS7CZZ32WyTBfKBk1KV1TRpCRppzIvU0eI3bQZpUTcRcO9TG52ztLrpSaaJPIBVJ8CtgWuBS4n6y75vvzrMr4LPA/8HPgzso/Rk4Hj8ualtkn6P8AnyNpukbQS+JuI+HqpEsPOJe9vSNK/s34NPoBngR9HxD+XCT3M1432RyNJk1JE/DpFIk7cjFL/bqrVc5aYE/zIvgPcAlxD1jvnNmAhWbevsoMsZtfaciX9A1lC27Fsf2FJ5wKHAkdGxNL82Gzg7yTNiIi/ajd2qtol8KUGx2YA782bss7jHgBBAAAF/ElEQVRuM279L43hzo1W5S/HIWki/nPg+LpmlJsk/RFZc1CZBL+npF80OC5gdom4VpKbaEYg6d6I2K+w/xuyJPxyBbErazKoi/sgsF9EvFR3fCpwb0TsXiL2CoapaQNn5b1sKpO/j7grIvZv8/7fAv+PLNkckX9Nvn94RGxRQRkrawZL1Z6dshlF0k6NDgPbA+e4B834cQ2+BZK2YOjj/FPAxpI2AYiI50qELvbQgaFeOrX27OntBq5P7vmx1XkbetsiYtP6Y/nP5xSy7pMnlInf4HkDJccbFN9j1H9KaPSpoWWJmsGm1yd3gIh4JH9n065kzSjFT3V51+T3AO8GHib75GvjxAl+ZJuRdSkrZpm78z+DEh9BI6LtKQNGsEzSURHx38WDkt4IPFn1w/JBTl+R9CftxlDjuV22IHvXsbBE2W4pPGPL/FiZ/uS1WKmawVIl4mTNKJJ2JxuMdTKwnGzwmyLiDWXiWnluohlHkqYAHwJ2BX4BXFpRl8M5wA+AnzA0Z8rrgMPIXuC2nTCbPHMSWVNKW71oJD3M+gOHgixZ/Bj4q4h4cbh7R4gr4NPAh/PYE8hG4H61zACoVM1gklYBixudIntns0mbcZM1o+SfCm8lG5y2OD+2NCLc/j7OXIMfgaT31npxSDosIn5aOHd6RHytRPhvA2vJ/nH8AVmXu4+UKS9ARCzMu/G9J48psrbnDzZquhkNSe9scHgLsqkL/rXduBGRpHcO2QvLw4HX1frA5zXtb0j6WER8pd3AiZrB9mxwbF0ibjdo4maUPyKrwf9Y0g1kL21TDeKzUXANfgSp+lHn999X6EUzEbi9ipesTZ5XagBVHuOf6g7Vato3R8R/lCzfVmTjAubkcRcBl0TE0yVi3gMcExHP1h3fErgx2pwsTtJ/A59r0Ax2FHBuFc0TjRJxuxWKYZpRPh4RjWr2bcnfS70jf8YbySow3498Licbe67BjyxVP2rIau8ARER/yZeJ69QNoPoB8F/5/pnAAqDtBB8Rf1pFGetJOoxsXMFlZF32at0Db5f0x8VPTqM0qT65Q9YOnzcrtesM4AeSGjaDtRs0YXv2L8k+Kb6t0IzysZIx15P3Ivoe8L38ncoJwNmAE/w4cQ1+BIlr8ANArWudgKnAKkr2opH0A4YGUB1F1oQyGfhIBQOoPt3kdETEBW3GvQ34PxFxT93x/YFvRcTr24w77P+jkiNZdwW2AXZnqBlsIfAQ8HhELGkzbpL2bEnHk/3iOBSoNaP8Q8KmMesATvAjKLz0ErALQy/ASr30Sqmu6aePigZQ5fH+ssHhTYBTgVdFxLQ24ybpp133S3S9U8CUiGirFi/pOrKXk7+oO34Q8JmIeFubcZMmYjejbFic4Ecg6T/IZgd8nAYjHxOO7GxbqgFUDZ6zKdlL4VPJFgC5qN32ckkPAIdG3bzy+Uf9n0XEHmXLWyVJ90fE3sOcW/cLtkT85Im40IxyYkS8saq41jmc4EegbIWok8gWuriKbFKpUs0cqRVqresGUFFB008h/gyyofR/TJZ4/q4+MbcRcy7wAbKJ1mrjDF5LtojLpRHxrTLxqyZpcUTsOtpzbT7Lidja4gTforwf8Un5NoWhZdR+Na4FG2OSvgi8k2z5wksiYmWFsd8K/F+yNm3I2rS/GCUXzkhB0hXATRHx93XHTwXeFBEnjk/JzIY4wbdB0gHApcC+CUejti3VAKo89iDZQsr9rN9kVcmng24haWvg+2RrptYWzziI7GX28VF+Ijqz0pzgW5R3qTuWrAZ/FNkMk1dExLXjWrAGJF3F0ACqt5AtDVh6AFVKqXrnpCbpDUCtLX5hlFv/16xSTvAjkHQM2YuuPwRuJ+vVcG1UtIBGCmM9gKoKqXrnmG3IPNBpZOeQDcD5eJSbOXIsJRlAlVJEXFT7utA750/JfqFeNNx9ZjY81+B7UKoBVKml6J1jtiFzDb4HdeKL35HU9c7Zp8reOWYbKtfgrSO4d45Z9Zzgzcx61ITxLoCZmaXhBG9m1qOc4M3MepQTvJlZj3KCNzPrUf8f/KUY+e5dKlIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7636b0e5c0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building a heatmap #\n",
    "sns.heatmap(tag_transition_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DO1Px-p0t9bC"
   },
   "source": [
    "state transition matrix for those POS TAGS which have more than 50% probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:42.829591Z",
     "start_time": "2018-10-07T06:38:42.536575Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PTyAtGZPt9bD",
    "outputId": "a6f33b1a-dded-4c56-e1a9-461f904ff24a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x76363e2d68>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7636b506d8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(tag_transition_df[tag_transition_df>=0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T10:11:03.784192Z",
     "start_time": "2018-10-06T10:11:03.776210Z"
    },
    "colab_type": "text",
    "id": "JRzO5b-Vt9bG"
   },
   "source": [
    "```\n",
    "DET -> NOUN = 0.637293 ~ For eg : The man, A dog etc. ~ Intuitively also this seems quite right\n",
    "ADJ -> NOUN = 0.700901 ~ For eg : Tall man, small child ~ Intuitively also this seems quite right\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-usA8oPzt9bH"
   },
   "source": [
    "### Build the vanilla Viterbi based POS tagger\n",
    "vanilla_viterbi_pos_tagger function, utilized the functions built above to predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:38:58.502332Z",
     "start_time": "2018-10-07T06:38:58.493344Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xaSNZT2It9bI"
   },
   "outputs": [],
   "source": [
    "def vanilla_viterbi_pos_tagger(words,db=None):\n",
    "    if not db:\n",
    "        db = build_database(data_for_training)\n",
    "    states = list()\n",
    "    for key,word in enumerate(words):\n",
    "        probs = list()\n",
    "        for t2 in db[\"unique_tags\"]:\n",
    "            if key == 0:\n",
    "                t1 = \".\"\n",
    "            else:\n",
    "                t1=states[-1]\n",
    "            # Some debugging statements for performance\n",
    "            #s = datetime.now()\n",
    "            emission_prob = calculate_prob_word_given_tag(word,t2,db)\n",
    "            #e = datetime.now() - s\n",
    "            #print(\"For word = %s, emission prob time = %f\" %(word,(e.seconds + (e.microseconds/1e6))))\n",
    "            #s = datetime.now()\n",
    "            transition_prob = db[\"tag_transition_df\"].loc[t2,t1]\n",
    "            #e = datetime.now() - s\n",
    "            #print(\"For word = %s, trans prob time = %f\" %(word,(e.seconds + (e.microseconds/1e6))))\n",
    "            state_probability = emission_prob * transition_prob\n",
    "            probs.append(state_probability)\n",
    "        states.append(db[\"unique_tags\"][np.where(np.array(probs) == max(probs))[0][0]])\n",
    "    return zip(words,states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYbJ0mcvt9bK"
   },
   "source": [
    "1. We create a ```test_data_df``` which will have the word, actual tag and the predicted tags we will create going forward\n",
    "2. vanilla_viterbi predictions will be saved in ```test_data_df``` under column ```vanilla_pred_tag```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:08:44.037392Z",
     "start_time": "2018-10-07T06:39:04.953707Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5SKPpy8it9bL",
    "outputId": "a265dc06-4d39-41de-b7bc-d4376254617d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on index = 0\n",
      "Working on index = 1\n",
      "Working on index = 2\n",
      "Working on index = 3\n",
      "Working on index = 4\n",
      "Working on index = 5\n",
      "Working on index = 6\n",
      "Working on index = 7\n",
      "Working on index = 8\n",
      "Working on index = 9\n",
      "Working on index = 10\n",
      "Working on index = 11\n",
      "Working on index = 12\n",
      "Working on index = 13\n",
      "Working on index = 14\n",
      "Working on index = 15\n",
      "Working on index = 16\n",
      "Working on index = 17\n",
      "Working on index = 18\n",
      "Working on index = 19\n",
      "Working on index = 20\n",
      "Working on index = 21\n",
      "Working on index = 22\n",
      "Working on index = 23\n",
      "Working on index = 24\n",
      "Working on index = 25\n",
      "Working on index = 26\n",
      "Working on index = 27\n",
      "Working on index = 28\n",
      "Working on index = 29\n",
      "Working on index = 30\n",
      "Working on index = 31\n",
      "Working on index = 32\n",
      "Working on index = 33\n",
      "Working on index = 34\n",
      "Working on index = 35\n",
      "Working on index = 36\n",
      "Working on index = 37\n",
      "Working on index = 38\n",
      "Working on index = 39\n",
      "Working on index = 40\n",
      "Working on index = 41\n",
      "Working on index = 42\n",
      "Working on index = 43\n",
      "Working on index = 44\n",
      "Working on index = 45\n",
      "Working on index = 46\n",
      "Working on index = 47\n",
      "Working on index = 48\n",
      "Working on index = 49\n",
      "Working on index = 50\n",
      "Working on index = 51\n",
      "Working on index = 52\n",
      "Working on index = 53\n",
      "Working on index = 54\n",
      "Working on index = 55\n",
      "Working on index = 56\n",
      "Working on index = 57\n",
      "Working on index = 58\n",
      "Working on index = 59\n",
      "Working on index = 60\n",
      "Working on index = 61\n",
      "Working on index = 62\n",
      "Working on index = 63\n",
      "Working on index = 64\n",
      "Working on index = 65\n",
      "Working on index = 66\n",
      "Working on index = 67\n",
      "Working on index = 68\n",
      "Working on index = 69\n",
      "Working on index = 70\n",
      "Working on index = 71\n",
      "Working on index = 72\n",
      "Working on index = 73\n",
      "Working on index = 74\n",
      "Working on index = 75\n",
      "Working on index = 76\n",
      "Working on index = 77\n",
      "Working on index = 78\n",
      "Working on index = 79\n",
      "Working on index = 80\n",
      "Working on index = 81\n",
      "Working on index = 82\n",
      "Working on index = 83\n",
      "Working on index = 84\n",
      "Working on index = 85\n",
      "Working on index = 86\n",
      "Working on index = 87\n",
      "Working on index = 88\n",
      "Working on index = 89\n",
      "Working on index = 90\n",
      "Working on index = 91\n",
      "Working on index = 92\n",
      "Working on index = 93\n",
      "Working on index = 94\n",
      "Working on index = 95\n",
      "Working on index = 96\n",
      "Working on index = 97\n",
      "Working on index = 98\n",
      "Working on index = 99\n",
      "Working on index = 100\n",
      "Working on index = 101\n",
      "Working on index = 102\n",
      "Working on index = 103\n",
      "Working on index = 104\n",
      "Working on index = 105\n",
      "Working on index = 106\n",
      "Working on index = 107\n",
      "Working on index = 108\n",
      "Working on index = 109\n",
      "Working on index = 110\n",
      "Working on index = 111\n",
      "Working on index = 112\n",
      "Working on index = 113\n",
      "Working on index = 114\n",
      "Working on index = 115\n",
      "Working on index = 116\n",
      "Working on index = 117\n",
      "Working on index = 118\n",
      "Working on index = 119\n",
      "Working on index = 120\n",
      "Working on index = 121\n",
      "Working on index = 122\n",
      "Working on index = 123\n",
      "Working on index = 124\n",
      "Working on index = 125\n",
      "Working on index = 126\n",
      "Working on index = 127\n",
      "Working on index = 128\n",
      "Working on index = 129\n",
      "Working on index = 130\n",
      "Working on index = 131\n",
      "Working on index = 132\n",
      "Working on index = 133\n",
      "Working on index = 134\n",
      "Working on index = 135\n",
      "Working on index = 136\n",
      "Working on index = 137\n",
      "Working on index = 138\n",
      "Working on index = 139\n",
      "Working on index = 140\n",
      "Working on index = 141\n",
      "Working on index = 142\n",
      "Working on index = 143\n",
      "Working on index = 144\n",
      "Working on index = 145\n",
      "Working on index = 146\n",
      "Working on index = 147\n",
      "Working on index = 148\n",
      "Working on index = 149\n",
      "Working on index = 150\n",
      "Working on index = 151\n",
      "Working on index = 152\n",
      "Working on index = 153\n",
      "Working on index = 154\n",
      "Working on index = 155\n",
      "Working on index = 156\n",
      "Working on index = 157\n",
      "Working on index = 158\n",
      "Working on index = 159\n",
      "Working on index = 160\n",
      "Working on index = 161\n",
      "Working on index = 162\n",
      "Working on index = 163\n",
      "Working on index = 164\n",
      "Working on index = 165\n",
      "Working on index = 166\n",
      "Working on index = 167\n",
      "Working on index = 168\n",
      "Working on index = 169\n",
      "Working on index = 170\n",
      "Working on index = 171\n",
      "Working on index = 172\n",
      "Working on index = 173\n",
      "Working on index = 174\n",
      "Working on index = 175\n",
      "Working on index = 176\n",
      "Working on index = 177\n",
      "Working on index = 178\n",
      "Working on index = 179\n",
      "Working on index = 180\n",
      "Working on index = 181\n",
      "Working on index = 182\n",
      "Working on index = 183\n",
      "Working on index = 184\n",
      "Working on index = 185\n",
      "Working on index = 186\n",
      "Working on index = 187\n",
      "Working on index = 188\n",
      "Working on index = 189\n",
      "Working on index = 190\n",
      "Working on index = 191\n",
      "Working on index = 192\n",
      "Working on index = 193\n",
      "Working on index = 194\n",
      "Working on index = 195\n"
     ]
    }
   ],
   "source": [
    "test_data_df = pd.DataFrame({\"word\" : [ word[0] for sent in test_data for word in sent],\n",
    "                             \"actual_tag\" : [word[1] for sent in test_data for word in sent]})\n",
    "predicted_state = list()\n",
    "for index,sent in enumerate(test_data):\n",
    "    print(\"Working on index = %d\" %(index))\n",
    "    pred_tup = vanilla_viterbi_pos_tagger([tup[0] for tup in sent],database)\n",
    "    for tup in pred_tup:\n",
    "        predicted_state.append(tup[1])\n",
    "test_data_df = test_data_df.assign(vanilla_pred_tag = predicted_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:33:29.794422Z",
     "start_time": "2018-10-07T07:33:29.781412Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tWnU42KKt9bO",
    "outputId": "319da893-c293-46b4-b57d-6585e48896c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_tag</th>\n",
       "      <th>word</th>\n",
       "      <th>vanilla_pred_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>Investors</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VERB</td>\n",
       "      <td>took</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>advantage</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADP</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual_tag       word vanilla_pred_tag\n",
       "0       NOUN  Investors             NOUN\n",
       "1       VERB       took             VERB\n",
       "2       NOUN  advantage             NOUN\n",
       "3        ADP         of              ADP\n",
       "4       NOUN    Tuesday             NOUN"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqQxCW-0t9bR"
   },
   "source": [
    "#### PREDICTION EVALUATION REPORT - VANILLA VITERBI HEURISTIC POS TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:33:32.106659Z",
     "start_time": "2018-10-07T07:33:31.971898Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Cg5HWZ4_t9bS",
    "outputId": "b2ceec59-d7aa-45fa-82cf-a628c44a31ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .       1.00      1.00      1.00       566\n",
      "        ADJ       0.87      0.74      0.80       292\n",
      "        ADP       0.94      0.98      0.96       424\n",
      "        ADV       0.92      0.82      0.87       167\n",
      "       CONJ       1.00      1.00      1.00       108\n",
      "        DET       0.98      0.96      0.97       407\n",
      "       NOUN       0.97      0.85      0.91      1328\n",
      "        NUM       0.99      0.81      0.89       180\n",
      "       PRON       0.98      1.00      0.99       127\n",
      "        PRT       0.94      0.97      0.96       171\n",
      "       VERB       0.95      0.88      0.91       645\n",
      "          X       0.51      1.00      0.68       312\n",
      "\n",
      "avg / total       0.93      0.91      0.91      4727\n",
      "\n",
      "Average Accuracy = 0.905014\n",
      "Rows = Actual Tags\n",
      "Columns = Predicted Tag\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>55</td>\n",
       "      <td>565</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>151</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1132</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.852410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.820359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>566</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X  VERB  PRT  PRON  NUM  NOUN  DET  CONJ  ADV  ADP  ADJ    .  accuracy\n",
       "X     312     0    0     0    0     0    0     0    0    0    0    0  1.000000\n",
       "VERB   55   565    1     0    0    19    0     0    0    0    5    0  0.875969\n",
       "PRT     0     0  166     0    0     0    0     0    2    3    0    0  0.970760\n",
       "PRON    0     0    0   127    0     0    0     0    0    0    0    0  1.000000\n",
       "NUM    34     0    0     0  145     0    1     0    0    0    0    0  0.805556\n",
       "NOUN  151    26    0     0    1  1132    3     0    0    0   15    0  0.852410\n",
       "DET     1     0    0     2    0     1  389     0    0   14    0    0  0.955774\n",
       "CONJ    0     0    0     0    0     0    0   108    0    0    0    0  1.000000\n",
       "ADV     4     0    3     0    0     2    2     0  137    8   11    0  0.820359\n",
       "ADP     1     0    4     0    0     0    1     0    3  415    0    0  0.978774\n",
       "ADJ    51     1    2     0    0    14    0     0    7    1  216    0  0.739726\n",
       ".       0     0    0     0    0     0    0     0    0    0    0  566  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(test_data_df[\"actual_tag\"],test_data_df[\"vanilla_pred_tag\"]))\n",
    "print(\"Average Accuracy = %f\" %(accuracy_score(test_data_df[\"actual_tag\"],test_data_df[\"vanilla_pred_tag\"])))\n",
    "viterbi_vanilla_pred_result = pd.DataFrame(confusion_matrix(test_data_df[\"actual_tag\"],test_data_df[\"vanilla_pred_tag\"],labels=database[\"unique_tags\"]),\n",
    "                                           columns = database[\"unique_tags\"],index=database[\"unique_tags\"]).assign(accuracy = [ test_data_df[(test_data_df[\"actual_tag\"] == tag) & (test_data_df[\"vanilla_pred_tag\"] == tag)].shape[0] / test_data_df[test_data_df[\"actual_tag\"] == tag].shape[0] for tag in database[\"unique_tags\"]])\n",
    "print(\"Rows = Actual Tags\\nColumns = Predicted Tag\")\n",
    "viterbi_vanilla_pred_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYz_GfWLt9bW"
   },
   "source": [
    "Tags whose accuracy is less than 90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:33:35.966737Z",
     "start_time": "2018-10-07T07:33:35.949722Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Rg3s-BSlt9bX",
    "outputId": "d876fb06-475f-481c-c72f-978d637e7172"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>55</td>\n",
       "      <td>565</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>151</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1132</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.852410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.820359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X  VERB  PRT  PRON  NUM  NOUN  DET  CONJ  ADV  ADP  ADJ  .  accuracy\n",
       "VERB   55   565    1     0    0    19    0     0    0    0    5  0  0.875969\n",
       "NUM    34     0    0     0  145     0    1     0    0    0    0  0  0.805556\n",
       "NOUN  151    26    0     0    1  1132    3     0    0    0   15  0  0.852410\n",
       "ADV     4     0    3     0    0     2    2     0  137    8   11  0  0.820359\n",
       "ADJ    51     1    2     0    0    14    0     0    7    1  216  0  0.739726"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_vanilla_pred_result[viterbi_vanilla_pred_result[\"accuracy\"] < 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAukJ-r6t9ba"
   },
   "source": [
    "#### Analysis of Vanilla Viterbi POS Tagger \n",
    "\n",
    "1. Maximum mis classification are getting tagged as \"X\"\n",
    "2. \"X\" is the first tag our list of unique_tags\n",
    "\n",
    "Lets check the words which are wrongly predicted as \"X\", if they were present in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:33:45.153940Z",
     "start_time": "2018-10-07T07:33:44.995712Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "h2LrokCHt9bc",
    "outputId": "df82556a-98c5-4008-d4db-5d5ccd8e4a2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words wrongly predicted, which are present in vocab =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Major mis predictions are getting tagged as X - Unknown words\n",
    "# Lets check if these words are missing in vocab or there is mis calculation in HMM algorithm #\n",
    "print(\"Words wrongly predicted, which are present in vocab =\")\n",
    "list(filter(lambda x : x in database[\"vocab\"], np.unique(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"vanilla_pred_tag\"] == \"X\")][\"word\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWYcEx_pt9bf"
   },
   "source": [
    "So the words being tagged incorrectly are the ones which are not present in the vocab.\n",
    "\n",
    "Lets look at the words which were tagged incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:33:48.178528Z",
     "start_time": "2018-10-07T07:33:48.157516Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TFLHgi_It9bg",
    "outputId": "14fbed71-7449-4101-95bb-6df253e6e51d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing in vocab and wrongly predicted as \"X\" = 297\n",
      "Words which are wrongly predicted as \"X\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([\"'82\", '11.95', '115', '12.52', '126,000', '13.90', '14.75',\n",
       "       '141.9', '143.08', '143.93', '149.9', '150.00', '1997', '221.4',\n",
       "       '240-page', '27-year', '29year', '3.19', '306', '472', '51.6',\n",
       "       '550,000', '59-year-old', '609', '618.1', '62-year-old',\n",
       "       '63-year-old', '692', '77.6', '8.75', '84-year-old', '858,000',\n",
       "       '879', '88.32', '89.7', '94.8', '960', '99.1', 'ACCOUNT',\n",
       "       'APPEARS', 'Above', 'Aerospace', 'Ala', 'Anderson', 'Anku',\n",
       "       'Aquino', 'Birmingham', 'Bolivia', 'Braun', 'Brussels',\n",
       "       'Californian', 'Cataracts', 'Chilver', 'Clays', 'Connections',\n",
       "       'Corazon', 'CoreStates', 'DISCOUNT', 'DeFazio', 'ECONOMIC',\n",
       "       'Earle', 'Five', 'Four', 'GROWTH', 'Gardner', 'Green', 'Hubbard',\n",
       "       'Ilminster', 'Indexing', 'Iran-Contra', 'Kelli', 'Ketchum',\n",
       "       'Kirkpatrick', 'Landrieu', 'Left', 'Louisiana-Pacific', 'Lucille',\n",
       "       'Malta', 'Mead', 'Metal', 'Milan', 'Mont', 'Moon', 'N.Y', 'NCNB',\n",
       "       'NEW', 'Next', 'Nylev', 'Old-House', 'Palmer', 'Paper', 'Paris',\n",
       "       'Peck', 'Philippine', 'Preston', 'Proper', 'R.D.', 'Researchers',\n",
       "       'S.A', 'Sandoz', 'Silverman', 'Sit', 'Somerset', 'Spending',\n",
       "       'Spiro', 'Sumitomo', 'Tiny', 'Tip', 'Tots', 'Trans',\n",
       "       'U.S.-Japanese', 'Video', 'Vos', 'Wick', 'Z.', 'abortion-related',\n",
       "       'accordance', 'accountants', 'administer', 'adults',\n",
       "       'aftereffects', 'alumni', 'amazingly', 'ankle', 'applicable',\n",
       "       'argues', 'arrows', 'assaults', 'athletic', 'attendance', 'awaits',\n",
       "       'barometer', 'behemoth', 'blinks', 'bombers', 'bricks',\n",
       "       'broadened', 'broadly', 'burn', 'burned', 'cancer-causing',\n",
       "       'cardiovascular', 'cavernous', 'chest', 'clamped',\n",
       "       'cleaner-burning', 'cleanup', 'clouding', 'construed', 'criteria',\n",
       "       'crying', 'cultivated', 'curbed', 'defeats', 'delayed',\n",
       "       'descending', 'deteriorating', 'diagnosed', 'diagram',\n",
       "       'directorship', 'disapproval', 'disapproved', 'discretion',\n",
       "       'display', 'dreamed', 'dust-up', 'emasculate', 'escape', 'espouse',\n",
       "       'ethanol', 'executive-office', 'exits', 'eye', 'facade', 'faded',\n",
       "       'father-in-law', 'faultlessly', 'fifth-largest', 'filled',\n",
       "       'finest', 'fleet', 'flirted', 'fluctuations', 'foam',\n",
       "       'foldability', 'forest-product', 'fuels', 'genie', 'glory',\n",
       "       'grandstander', 'headed', 'heebie-jeebies', 'heirs', 'high-rise',\n",
       "       'highest-pitched', 'hotels', 'housewife', 'ignored', 'illustrates',\n",
       "       'impressive', 'improbable', 'incisions', 'infectious',\n",
       "       'information-services', 'initiate', 'inkling', 'insane',\n",
       "       'insanity', 'inserted', 'insurance-company', 'intelligent',\n",
       "       'intervention', 'intoxication', 'intricate', 'investor-relations',\n",
       "       'jeopardy', 'jolts', 'le', 'len', 'leveraging', 'loose', 'luxury',\n",
       "       'malnutrition', 'manpower', 'marketing-communications',\n",
       "       'marvelously', 'masters', 'maturing', 'midyear', 'money-losing',\n",
       "       'movies', 'mushy', 'nine-month', 'non-biodegradable',\n",
       "       'nonresidential', 'outlawed', 'outlawing', 'overvalued', 'owning',\n",
       "       'polyester', 'polyproplene', 'polystyrene', 'portrayal',\n",
       "       'preclinical', 'projector', 'properties', 'prosecutor',\n",
       "       'purhasing', 'pushes', 'readings', 'reclaimed', 'refer',\n",
       "       'relevance', 'removing', 'ritual', 'round-trip', 'rounds',\n",
       "       'safeguarding', 'sagged', 'savvier', 'select', 'seventh',\n",
       "       'shipboard', 'shop', 'shopped', 'shown', 'sight', 'smiles',\n",
       "       'soaring', 'solemn', 'speculate', 'stemmed', 'swallow',\n",
       "       'tailor-made', 'tempted', 'third-largest', 'underline',\n",
       "       'underprivileged', 'unrestricted', 'unsolicited', 'unwilling',\n",
       "       'usurp', 'vague', 'viewpoints', 'wealthy', 'well-connected',\n",
       "       'wrath', 'wrestling', 'yellow', 'yen-support'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the main problem is that the words which are not present, are being predicted as X #\n",
    "# So we need to form a policy which could enable missing words to be correctly predicted #\n",
    "# Let see the words which are wrongly predicted\n",
    "print(\"Number of words missing in vocab and wrongly predicted as \\\"X\\\" = %d\" %(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"vanilla_pred_tag\"] == \"X\")].shape[0]))\n",
    "print(\"Words which are wrongly predicted as \\\"X\\\"\")\n",
    "np.unique(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"vanilla_pred_tag\"] == \"X\")][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USLMepFYt9bv"
   },
   "source": [
    "Looking at the above list,, following patterns can be seen\n",
    "1. Numbers are being incorrectly tagged. We can solve this using a regex tagger as being NUM\n",
    "2. Words starting with CAPITAL letter, which are not present in the vocab are being tagged incorrectly. We can solve this also by using regex based tagger as being NOUN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7SrsM0et9bw"
   },
   "source": [
    "### Solve the problem of Unknown Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lur5SHpt9bx"
   },
   "source": [
    "### STRATEGY 1 (Lexical + Syntactic Based)\n",
    "```viterbi_backoff``` function acts as a backoff function for vanilla viterbi. This means, if a word is not found in the vocab then it uses following strategy for tagging the word\n",
    "\n",
    "**Check 1** : The word is checked using regex tagger to be a number, if true then ```NUM``` Tag is returned\n",
    "\n",
    "``` back's off too```\n",
    "\n",
    "**Check 2** :  The word is checked using regex tagger if it starts with a capital letter, if true, then ```NOUN``` is returned\n",
    "\n",
    "``` back's off too```\n",
    "\n",
    "**Check 3** : It uses **Snowball stemmer** based Bigram tagger to tag the word\n",
    "\n",
    "``` back's off too```\n",
    "\n",
    "**Check 4** :  It uses **Snowball stemmer** based Unigram tagger to tag the word\n",
    "\n",
    "``` back's off too```\n",
    "\n",
    "**Check 5** :  If none of the above works, then it returns ```X``` again.\n",
    "\n",
    "### STRATEGY 2 (Use only transition probability)\n",
    "Another strategy is to use only transition probability when a word is not present in the vocab.\n",
    "\n",
    "For realizing both the above strategy, we will modify the viterbi pos tagger function as mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:33:50.873093Z",
     "start_time": "2018-10-07T07:33:50.865087Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "k-Cy3ytGt9by"
   },
   "outputs": [],
   "source": [
    "def viterbi_backoff(test_word,db):\n",
    "    stem_obj = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    test_word_stemmed = stem_obj.stem(test_word)\n",
    "    \n",
    "    default_reg_ex_pattern = [('.*','X')]\n",
    "    default_reg_ex_tagger = nltk.RegexpTagger(default_reg_ex_pattern)\n",
    "    \n",
    "    unigram_tagger = nltk.UnigramTagger(db[\"stemmed_base_data\"],backoff=default_reg_ex_tagger)\n",
    "    \n",
    "    bigram_tagger = nltk.BigramTagger(db[\"stemmed_base_data\"],backoff=unigram_tagger)\n",
    "    \n",
    "    reg_ex_pattern = [('^[0-9]+.*','NUM'),\n",
    "                      ('^[A-Z]+[\\-\\.\\_\\\\]*[A-z]+$',\"NOUN\")] # remember : we do not add a default case,as we need to know if the prediction failed\n",
    "    regex_tagger = nltk.RegexpTagger(reg_ex_pattern)\n",
    "    \n",
    "    tag_pred = regex_tagger.tag([test_word])\n",
    "    \n",
    "    if not tag_pred[0][1]:\n",
    "        tag_pred = bigram_tagger.tag([test_word_stemmed])\n",
    "    \n",
    "    if tag_pred[0][1] == \"X\":\n",
    "        return False,tag_pred[0][1]\n",
    "    else:\n",
    "        return True,tag_pred[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:33:53.035319Z",
     "start_time": "2018-10-07T07:33:53.027322Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kvGlTxWBt9b7"
   },
   "outputs": [],
   "source": [
    "def modified_vanilla_viterbi_pos_tagger(words,db=None,strategy=\"backoff\"):\n",
    "    if not db:\n",
    "        db = build_database(data_for_training)\n",
    "    states = list()\n",
    "    # debug #\n",
    "    #print(\"using strategy = %s\" %(strategy))\n",
    "    for key,word in enumerate(words):\n",
    "        if word in db[\"vocab\"] or strategy == \"unknown_use_transition_only\":\n",
    "            probs = list()\n",
    "            for t2 in db[\"unique_tags\"]:\n",
    "                if key == 0:\n",
    "                    t1 = \".\"\n",
    "                else:\n",
    "                    t1=states[-1]\n",
    "                if word in db[\"vocab\"]:\n",
    "                    emission_prob = calculate_prob_word_given_tag(word,t2,db)\n",
    "                else:\n",
    "                    #print(\"force selection emission = 1 for word = %s\" %(word))\n",
    "                    emission_prob = 1.0\n",
    "                transition_prob = db[\"tag_transition_df\"].loc[t2,t1]\n",
    "                state_probability = emission_prob * transition_prob\n",
    "                probs.append(state_probability)\n",
    "            states.append(db[\"unique_tags\"][np.where(np.array(probs) == max(probs))[0][0]])\n",
    "        else:\n",
    "            #print(\"backing off for word = %s\" %(word))\n",
    "            result, tag_pred = viterbi_backoff(word,db)\n",
    "            states.append(tag_pred)\n",
    "    return zip(words,states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0WhC1bSt9b_"
   },
   "source": [
    "Lets try modified viterbi on a test sentence.\n",
    "\n",
    "*Test sentence = \"It rained 223mm in Japan yesterday, very heavy rainfall\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:34:08.130770Z",
     "start_time": "2018-10-07T07:33:56.436623Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jhAwXztct9cA",
    "outputId": "9abb0b87-5e0d-4125-d9ad-a807a9f846f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence = It rained 223mm in Japan yesterday, very heavy rainfall\n",
      "\n",
      "Using vanilla viterbi, the tags are:\n",
      "('It', 'PRON')\n",
      "('rained', 'X')\n",
      "('223mm', 'X')\n",
      "('in', 'ADP')\n",
      "('Japan', 'NOUN')\n",
      "('yesterday,', 'X')\n",
      "('very', 'ADV')\n",
      "('heavy', 'ADJ')\n",
      "('rainfall', 'X')\n",
      "\n",
      "Using modified viterbi (strategy = backoff), the tags are:\n",
      "('It', 'PRON')\n",
      "('rained', 'NOUN')\n",
      "('223mm', 'NUM')\n",
      "('in', 'ADP')\n",
      "('Japan', 'NOUN')\n",
      "('yesterday,', 'X')\n",
      "('very', 'ADV')\n",
      "('heavy', 'ADJ')\n",
      "('rainfall', 'X')\n",
      "\n",
      " Using modified viterbi (strategy = unknown_use_transition_only), the tags are:\n",
      "('It', 'PRON')\n",
      "('rained', 'VERB')\n",
      "('223mm', 'X')\n",
      "('in', 'ADP')\n",
      "('Japan', 'NOUN')\n",
      "('yesterday,', 'NOUN')\n",
      "('very', 'ADV')\n",
      "('heavy', 'ADJ')\n",
      "('rainfall', 'NOUN')\n"
     ]
    }
   ],
   "source": [
    "original_sent = \"It rained 223mm in Japan yesterday, very heavy rainfall\"\n",
    "print(\"Original sentence = %s\" %(original_sent))\n",
    "print(\"\\nUsing vanilla viterbi, the tags are:\")\n",
    "for x in vanilla_viterbi_pos_tagger(original_sent.split(),database):\n",
    "    print(x)\n",
    "print(\"\\nUsing modified viterbi (strategy = backoff), the tags are:\")\n",
    "for x in modified_vanilla_viterbi_pos_tagger(\"It rained 223mm in Japan yesterday, very heavy rainfall\".split(),database,strategy=\"backoff\"):\n",
    "    print(x)\n",
    "print(\"\\n Using modified viterbi (strategy = unknown_use_transition_only), the tags are:\")\n",
    "for x in modified_vanilla_viterbi_pos_tagger(\"It rained 223mm in Japan yesterday, very heavy rainfall\".split(),database,strategy=\"unknown_use_transition_only\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FY1nARczt9cE"
   },
   "source": [
    "**Lets run on test data using ```strategy=\"backoff\"```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:09:23.838539Z",
     "start_time": "2018-10-07T07:34:37.345313Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lT41z3d6t9cF",
    "outputId": "bec669a8-b3a4-4654-e8df-044e2844e490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on index = 0\n",
      "Working on index = 1\n",
      "Working on index = 2\n",
      "Working on index = 3\n",
      "Working on index = 4\n",
      "Working on index = 5\n",
      "Working on index = 6\n",
      "Working on index = 7\n",
      "Working on index = 8\n",
      "Working on index = 9\n",
      "Working on index = 10\n",
      "Working on index = 11\n",
      "Working on index = 12\n",
      "Working on index = 13\n",
      "Working on index = 14\n",
      "Working on index = 15\n",
      "Working on index = 16\n",
      "Working on index = 17\n",
      "Working on index = 18\n",
      "Working on index = 19\n",
      "Working on index = 20\n",
      "Working on index = 21\n",
      "Working on index = 22\n",
      "Working on index = 23\n",
      "Working on index = 24\n",
      "Working on index = 25\n",
      "Working on index = 26\n",
      "Working on index = 27\n",
      "Working on index = 28\n",
      "Working on index = 29\n",
      "Working on index = 30\n",
      "Working on index = 31\n",
      "Working on index = 32\n",
      "Working on index = 33\n",
      "Working on index = 34\n",
      "Working on index = 35\n",
      "Working on index = 36\n",
      "Working on index = 37\n",
      "Working on index = 38\n",
      "Working on index = 39\n",
      "Working on index = 40\n",
      "Working on index = 41\n",
      "Working on index = 42\n",
      "Working on index = 43\n",
      "Working on index = 44\n",
      "Working on index = 45\n",
      "Working on index = 46\n",
      "Working on index = 47\n",
      "Working on index = 48\n",
      "Working on index = 49\n",
      "Working on index = 50\n",
      "Working on index = 51\n",
      "Working on index = 52\n",
      "Working on index = 53\n",
      "Working on index = 54\n",
      "Working on index = 55\n",
      "Working on index = 56\n",
      "Working on index = 57\n",
      "Working on index = 58\n",
      "Working on index = 59\n",
      "Working on index = 60\n",
      "Working on index = 61\n",
      "Working on index = 62\n",
      "Working on index = 63\n",
      "Working on index = 64\n",
      "Working on index = 65\n",
      "Working on index = 66\n",
      "Working on index = 67\n",
      "Working on index = 68\n",
      "Working on index = 69\n",
      "Working on index = 70\n",
      "Working on index = 71\n",
      "Working on index = 72\n",
      "Working on index = 73\n",
      "Working on index = 74\n",
      "Working on index = 75\n",
      "Working on index = 76\n",
      "Working on index = 77\n",
      "Working on index = 78\n",
      "Working on index = 79\n",
      "Working on index = 80\n",
      "Working on index = 81\n",
      "Working on index = 82\n",
      "Working on index = 83\n",
      "Working on index = 84\n",
      "Working on index = 85\n",
      "Working on index = 86\n",
      "Working on index = 87\n",
      "Working on index = 88\n",
      "Working on index = 89\n",
      "Working on index = 90\n",
      "Working on index = 91\n",
      "Working on index = 92\n",
      "Working on index = 93\n",
      "Working on index = 94\n",
      "Working on index = 95\n",
      "Working on index = 96\n",
      "Working on index = 97\n",
      "Working on index = 98\n",
      "Working on index = 99\n",
      "Working on index = 100\n",
      "Working on index = 101\n",
      "Working on index = 102\n",
      "Working on index = 103\n",
      "Working on index = 104\n",
      "Working on index = 105\n",
      "Working on index = 106\n",
      "Working on index = 107\n",
      "Working on index = 108\n",
      "Working on index = 109\n",
      "Working on index = 110\n",
      "Working on index = 111\n",
      "Working on index = 112\n",
      "Working on index = 113\n",
      "Working on index = 114\n",
      "Working on index = 115\n",
      "Working on index = 116\n",
      "Working on index = 117\n",
      "Working on index = 118\n",
      "Working on index = 119\n",
      "Working on index = 120\n",
      "Working on index = 121\n",
      "Working on index = 122\n",
      "Working on index = 123\n",
      "Working on index = 124\n",
      "Working on index = 125\n",
      "Working on index = 126\n",
      "Working on index = 127\n",
      "Working on index = 128\n",
      "Working on index = 129\n",
      "Working on index = 130\n",
      "Working on index = 131\n",
      "Working on index = 132\n",
      "Working on index = 133\n",
      "Working on index = 134\n",
      "Working on index = 135\n",
      "Working on index = 136\n",
      "Working on index = 137\n",
      "Working on index = 138\n",
      "Working on index = 139\n",
      "Working on index = 140\n",
      "Working on index = 141\n",
      "Working on index = 142\n",
      "Working on index = 143\n",
      "Working on index = 144\n",
      "Working on index = 145\n",
      "Working on index = 146\n",
      "Working on index = 147\n",
      "Working on index = 148\n",
      "Working on index = 149\n",
      "Working on index = 150\n",
      "Working on index = 151\n",
      "Working on index = 152\n",
      "Working on index = 153\n",
      "Working on index = 154\n",
      "Working on index = 155\n",
      "Working on index = 156\n",
      "Working on index = 157\n",
      "Working on index = 158\n",
      "Working on index = 159\n",
      "Working on index = 160\n",
      "Working on index = 161\n",
      "Working on index = 162\n",
      "Working on index = 163\n",
      "Working on index = 164\n",
      "Working on index = 165\n",
      "Working on index = 166\n",
      "Working on index = 167\n",
      "Working on index = 168\n",
      "Working on index = 169\n",
      "Working on index = 170\n",
      "Working on index = 171\n",
      "Working on index = 172\n",
      "Working on index = 173\n",
      "Working on index = 174\n",
      "Working on index = 175\n",
      "Working on index = 176\n",
      "Working on index = 177\n",
      "Working on index = 178\n",
      "Working on index = 179\n",
      "Working on index = 180\n",
      "Working on index = 181\n",
      "Working on index = 182\n",
      "Working on index = 183\n",
      "Working on index = 184\n",
      "Working on index = 185\n",
      "Working on index = 186\n",
      "Working on index = 187\n",
      "Working on index = 188\n",
      "Working on index = 189\n",
      "Working on index = 190\n",
      "Working on index = 191\n",
      "Working on index = 192\n",
      "Working on index = 193\n",
      "Working on index = 194\n",
      "Working on index = 195\n"
     ]
    }
   ],
   "source": [
    "predicted_state = list()\n",
    "for index,sent in enumerate(test_data):\n",
    "    print(\"Working on index = %d\" %(index))\n",
    "    pred_tup = modified_vanilla_viterbi_pos_tagger([tup[0] for tup in sent],database,strategy=\"backoff\")\n",
    "    for tup in pred_tup:\n",
    "        predicted_state.append(tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:15:06.149810Z",
     "start_time": "2018-10-07T08:15:06.143804Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kVRzeFpPt9cK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_df = test_data_df.assign(backoff_tag_pred = predicted_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfMg28S4t9cN"
   },
   "source": [
    "#### Analysis of modified viterbi (```strategy = backoff```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:15:08.302733Z",
     "start_time": "2018-10-07T08:15:08.133554Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XoFfpmaAt9cN",
    "outputId": "b66ad1b0-5b79-4c45-e80d-e90ca6955422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .       1.00      1.00      1.00       566\n",
      "        ADJ       0.86      0.74      0.80       292\n",
      "        ADP       0.94      0.98      0.96       424\n",
      "        ADV       0.93      0.82      0.87       167\n",
      "       CONJ       1.00      1.00      1.00       108\n",
      "        DET       0.98      0.96      0.97       407\n",
      "       NOUN       0.96      0.92      0.94      1328\n",
      "        NUM       0.96      0.98      0.97       180\n",
      "       PRON       0.98      1.00      0.99       127\n",
      "        PRT       0.95      0.97      0.96       171\n",
      "       VERB       0.95      0.91      0.93       645\n",
      "          X       0.73      1.00      0.85       312\n",
      "\n",
      "avg / total       0.94      0.94      0.94      4727\n",
      "\n",
      "Average Accuracy = 0.936112\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>24</td>\n",
       "      <td>584</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.905426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1228</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.924699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.820359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>0.743151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>566</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X  VERB  PRT  PRON  NUM  NOUN  DET  CONJ  ADV  ADP  ADJ    .  accuracy\n",
       "X     312     0    0     0    0     0    0     0    0    0    0    0  1.000000\n",
       "VERB   24   584    1     0    0    29    0     0    0    0    7    0  0.905426\n",
       "PRT     0     0  166     0    0     0    0     0    2    3    0    0  0.970760\n",
       "PRON    0     0    0   127    0     0    0     0    0    0    0    0  1.000000\n",
       "NUM     1     0    0     0  176     2    1     0    0    0    0    0  0.977778\n",
       "NOUN   49    30    0     0    1  1228    3     0    0    0   17    0  0.924699\n",
       "DET     1     0    0     2    0     1  389     0    0   14    0    0  0.955774\n",
       "CONJ    0     0    0     0    0     0    0   108    0    0    0    0  1.000000\n",
       "ADV     3     0    2     0    0     2    2     0  137    9   12    0  0.820359\n",
       "ADP     0     0    4     0    0     1    1     0    3  415    0    0  0.978774\n",
       "ADJ    36     1    2     0    7    22    0     0    6    1  217    0  0.743151\n",
       ".       0     0    0     0    0     0    0     0    0    0    0  566  1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(test_data_df[\"actual_tag\"],test_data_df[\"backoff_tag_pred\"]))\n",
    "print(\"Average Accuracy = %f\" %(accuracy_score(test_data_df[\"actual_tag\"],test_data_df[\"backoff_tag_pred\"])))\n",
    "viterbi_backoff_pred_result = pd.DataFrame(confusion_matrix(test_data_df[\"actual_tag\"],test_data_df[\"backoff_tag_pred\"],labels=database[\"unique_tags\"]),\n",
    "                                           columns = database[\"unique_tags\"],index=database[\"unique_tags\"]).assign(accuracy = [ test_data_df[(test_data_df[\"actual_tag\"] == tag) & (test_data_df[\"backoff_tag_pred\"] == tag)].shape[0] / test_data_df[test_data_df[\"actual_tag\"] == tag].shape[0] for tag in database[\"unique_tags\"]])\n",
    "viterbi_backoff_pred_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRtsFVDjt9cQ"
   },
   "source": [
    "The net accuracy has increased from 90.5% to 93.6%.\n",
    "\n",
    "Let look at the words predicted as X, were they present in vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:15:12.796520Z",
     "start_time": "2018-10-07T08:15:12.673629Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZjQmDuDut9cV",
    "outputId": "8856e9a2-2c51-48aa-d491-25f86a262422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words wrongly predicted, which are present in vocab =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Words wrongly predicted, which are present in vocab =\")\n",
    "list(filter(lambda x : x in database[\"vocab\"], np.unique(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"backoff_tag_pred\"] == \"X\")][\"word\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KH-sXf-at9cY"
   },
   "source": [
    "So all words predicted as \"X\", were not present in the Vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:15:14.458330Z",
     "start_time": "2018-10-07T08:15:14.436316Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "f0VLogSxt9cZ",
    "outputId": "de8df9eb-d92e-4091-a95c-fe958e484b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing in vocab and wrongly predicted as \"X\" = 114\n",
      "Words which are wrongly predicted as \"X\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([\"'82\", 'abortion-related', 'administer', 'aftereffects', 'alumni',\n",
       "       'amazingly', 'ankle', 'arrows', 'athletic', 'awaits', 'barometer',\n",
       "       'behemoth', 'blinks', 'bricks', 'burn', 'burned', 'cancer-causing',\n",
       "       'cardiovascular', 'cavernous', 'chest', 'clamped',\n",
       "       'cleaner-burning', 'cleanup', 'construed', 'criteria',\n",
       "       'descending', 'diagnosed', 'diagram', 'directorship', 'discretion',\n",
       "       'dreamed', 'dust-up', 'emasculate', 'espouse', 'ethanol',\n",
       "       'executive-office', 'facade', 'faded', 'father-in-law',\n",
       "       'faultlessly', 'fifth-largest', 'finest', 'flirted', 'foam',\n",
       "       'glory', 'grandstander', 'heebie-jeebies', 'heirs', 'high-rise',\n",
       "       'highest-pitched', 'housewife', 'illustrates', 'improbable',\n",
       "       'incisions', 'infectious', 'information-services', 'inkling',\n",
       "       'insane', 'insanity', 'insurance-company', 'intoxication',\n",
       "       'intricate', 'investor-relations', 'jeopardy', 'jolts', 'le',\n",
       "       'loose', 'luxury', 'malnutrition', 'manpower',\n",
       "       'marketing-communications', 'marvelously', 'midyear',\n",
       "       'money-losing', 'mushy', 'nine-month', 'non-biodegradable',\n",
       "       'nonresidential', 'outlawed', 'outlawing', 'overvalued',\n",
       "       'polyester', 'polyproplene', 'polystyrene', 'preclinical',\n",
       "       'projector', 'purhasing', 'relevance', 'ritual', 'round-trip',\n",
       "       'safeguarding', 'savvier', 'seventh', 'shipboard', 'shown',\n",
       "       'smiles', 'solemn', 'swallow', 'tailor-made', 'third-largest',\n",
       "       'underline', 'underprivileged', 'unrestricted', 'unsolicited',\n",
       "       'unwilling', 'usurp', 'vague', 'wealthy', 'well-connected',\n",
       "       'wrath', 'wrestling', 'yellow', 'yen-support'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words missing in vocab and wrongly predicted as \\\"X\\\" = %d\" %(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"backoff_tag_pred\"] == \"X\")].shape[0]))\n",
    "print(\"Words which are wrongly predicted as \\\"X\\\"\")\n",
    "np.unique(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"backoff_tag_pred\"] == \"X\")][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "me4F80X9t9cg"
   },
   "source": [
    "So the number of words wrongly predicted as reduced from \n",
    "* vanilla viterbi -> 297 mis classifications \n",
    "* modified viterby - strategy backoff -> 114 mis classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5rRIYf1t9ch"
   },
   "source": [
    "**Lets run on test data using ```strategy=\"unknown_use_transition_only\"```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:43:27.983789Z",
     "start_time": "2018-10-07T08:15:18.898688Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YI1UqCzJt9cj",
    "outputId": "9ba6d32a-77a2-418f-d9bf-ed313000f2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on index = 0\n",
      "Working on index = 1\n",
      "Working on index = 2\n",
      "Working on index = 3\n",
      "Working on index = 4\n",
      "Working on index = 5\n",
      "Working on index = 6\n",
      "Working on index = 7\n",
      "Working on index = 8\n",
      "Working on index = 9\n",
      "Working on index = 10\n",
      "Working on index = 11\n",
      "Working on index = 12\n",
      "Working on index = 13\n",
      "Working on index = 14\n",
      "Working on index = 15\n",
      "Working on index = 16\n",
      "Working on index = 17\n",
      "Working on index = 18\n",
      "Working on index = 19\n",
      "Working on index = 20\n",
      "Working on index = 21\n",
      "Working on index = 22\n",
      "Working on index = 23\n",
      "Working on index = 24\n",
      "Working on index = 25\n",
      "Working on index = 26\n",
      "Working on index = 27\n",
      "Working on index = 28\n",
      "Working on index = 29\n",
      "Working on index = 30\n",
      "Working on index = 31\n",
      "Working on index = 32\n",
      "Working on index = 33\n",
      "Working on index = 34\n",
      "Working on index = 35\n",
      "Working on index = 36\n",
      "Working on index = 37\n",
      "Working on index = 38\n",
      "Working on index = 39\n",
      "Working on index = 40\n",
      "Working on index = 41\n",
      "Working on index = 42\n",
      "Working on index = 43\n",
      "Working on index = 44\n",
      "Working on index = 45\n",
      "Working on index = 46\n",
      "Working on index = 47\n",
      "Working on index = 48\n",
      "Working on index = 49\n",
      "Working on index = 50\n",
      "Working on index = 51\n",
      "Working on index = 52\n",
      "Working on index = 53\n",
      "Working on index = 54\n",
      "Working on index = 55\n",
      "Working on index = 56\n",
      "Working on index = 57\n",
      "Working on index = 58\n",
      "Working on index = 59\n",
      "Working on index = 60\n",
      "Working on index = 61\n",
      "Working on index = 62\n",
      "Working on index = 63\n",
      "Working on index = 64\n",
      "Working on index = 65\n",
      "Working on index = 66\n",
      "Working on index = 67\n",
      "Working on index = 68\n",
      "Working on index = 69\n",
      "Working on index = 70\n",
      "Working on index = 71\n",
      "Working on index = 72\n",
      "Working on index = 73\n",
      "Working on index = 74\n",
      "Working on index = 75\n",
      "Working on index = 76\n",
      "Working on index = 77\n",
      "Working on index = 78\n",
      "Working on index = 79\n",
      "Working on index = 80\n",
      "Working on index = 81\n",
      "Working on index = 82\n",
      "Working on index = 83\n",
      "Working on index = 84\n",
      "Working on index = 85\n",
      "Working on index = 86\n",
      "Working on index = 87\n",
      "Working on index = 88\n",
      "Working on index = 89\n",
      "Working on index = 90\n",
      "Working on index = 91\n",
      "Working on index = 92\n",
      "Working on index = 93\n",
      "Working on index = 94\n",
      "Working on index = 95\n",
      "Working on index = 96\n",
      "Working on index = 97\n",
      "Working on index = 98\n",
      "Working on index = 99\n",
      "Working on index = 100\n",
      "Working on index = 101\n",
      "Working on index = 102\n",
      "Working on index = 103\n",
      "Working on index = 104\n",
      "Working on index = 105\n",
      "Working on index = 106\n",
      "Working on index = 107\n",
      "Working on index = 108\n",
      "Working on index = 109\n",
      "Working on index = 110\n",
      "Working on index = 111\n",
      "Working on index = 112\n",
      "Working on index = 113\n",
      "Working on index = 114\n",
      "Working on index = 115\n",
      "Working on index = 116\n",
      "Working on index = 117\n",
      "Working on index = 118\n",
      "Working on index = 119\n",
      "Working on index = 120\n",
      "Working on index = 121\n",
      "Working on index = 122\n",
      "Working on index = 123\n",
      "Working on index = 124\n",
      "Working on index = 125\n",
      "Working on index = 126\n",
      "Working on index = 127\n",
      "Working on index = 128\n",
      "Working on index = 129\n",
      "Working on index = 130\n",
      "Working on index = 131\n",
      "Working on index = 132\n",
      "Working on index = 133\n",
      "Working on index = 134\n",
      "Working on index = 135\n",
      "Working on index = 136\n",
      "Working on index = 137\n",
      "Working on index = 138\n",
      "Working on index = 139\n",
      "Working on index = 140\n",
      "Working on index = 141\n",
      "Working on index = 142\n",
      "Working on index = 143\n",
      "Working on index = 144\n",
      "Working on index = 145\n",
      "Working on index = 146\n",
      "Working on index = 147\n",
      "Working on index = 148\n",
      "Working on index = 149\n",
      "Working on index = 150\n",
      "Working on index = 151\n",
      "Working on index = 152\n",
      "Working on index = 153\n",
      "Working on index = 154\n",
      "Working on index = 155\n",
      "Working on index = 156\n",
      "Working on index = 157\n",
      "Working on index = 158\n",
      "Working on index = 159\n",
      "Working on index = 160\n",
      "Working on index = 161\n",
      "Working on index = 162\n",
      "Working on index = 163\n",
      "Working on index = 164\n",
      "Working on index = 165\n",
      "Working on index = 166\n",
      "Working on index = 167\n",
      "Working on index = 168\n",
      "Working on index = 169\n",
      "Working on index = 170\n",
      "Working on index = 171\n",
      "Working on index = 172\n",
      "Working on index = 173\n",
      "Working on index = 174\n",
      "Working on index = 175\n",
      "Working on index = 176\n",
      "Working on index = 177\n",
      "Working on index = 178\n",
      "Working on index = 179\n",
      "Working on index = 180\n",
      "Working on index = 181\n",
      "Working on index = 182\n",
      "Working on index = 183\n",
      "Working on index = 184\n",
      "Working on index = 185\n",
      "Working on index = 186\n",
      "Working on index = 187\n",
      "Working on index = 188\n",
      "Working on index = 189\n",
      "Working on index = 190\n",
      "Working on index = 191\n",
      "Working on index = 192\n",
      "Working on index = 193\n",
      "Working on index = 194\n",
      "Working on index = 195\n"
     ]
    }
   ],
   "source": [
    "predicted_state = list()\n",
    "for index,sent in enumerate(test_data):\n",
    "    print(\"Working on index = %d\" %(index))\n",
    "    pred_tup = modified_vanilla_viterbi_pos_tagger([tup[0] for tup in sent],database,strategy=\"unknown_use_transition_only\")\n",
    "    for tup in pred_tup:\n",
    "        predicted_state.append(tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T09:40:55.085994Z",
     "start_time": "2018-10-07T09:40:55.079988Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LgfvDv7kt9cp"
   },
   "outputs": [],
   "source": [
    "test_data_df = test_data_df.assign(only_transition_tag_pred = predicted_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T09:40:56.762880Z",
     "start_time": "2018-10-07T09:40:56.618623Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MBsWXv2ut9cs",
    "outputId": "5e390304-02fe-4288-e1c0-386d89cdffd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .       1.00      1.00      1.00       566\n",
      "        ADJ       0.88      0.74      0.80       292\n",
      "        ADP       0.94      0.98      0.96       424\n",
      "        ADV       0.92      0.83      0.87       167\n",
      "       CONJ       1.00      1.00      1.00       108\n",
      "        DET       0.91      0.96      0.93       407\n",
      "       NOUN       0.92      0.94      0.93      1328\n",
      "        NUM       0.99      0.81      0.89       180\n",
      "       PRON       0.98      1.00      0.99       127\n",
      "        PRT       0.95      0.97      0.96       171\n",
      "       VERB       0.91      0.91      0.91       645\n",
      "          X       0.92      0.97      0.95       312\n",
      "\n",
      "avg / total       0.93      0.93      0.93      4727\n",
      "\n",
      "Average Accuracy = 0.933150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>304</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>14</td>\n",
       "      <td>584</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.905426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1252</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.826347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>0.743151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>566</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X  VERB  PRT  PRON  NUM  NOUN  DET  CONJ  ADV  ADP  ADJ    .  accuracy\n",
       "X     304     3    0     0    0     2    3     0    0    0    0    0  0.974359\n",
       "VERB   14   584    1     0    0    39    2     0    0    0    5    0  0.905426\n",
       "PRT     0     0  166     0    0     0    0     0    2    3    0    0  0.970760\n",
       "PRON    0     0    0   127    0     0    0     0    0    0    0    0  1.000000\n",
       "NUM     0     5    0     0  145    23    7     0    0    0    0    0  0.805556\n",
       "NOUN    7    37    0     0    1  1252   16     0    0    0   15    0  0.942771\n",
       "DET     0     0    0     2    0     2  389     0    0   14    0    0  0.955774\n",
       "CONJ    0     0    0     0    0     0    0   108    0    0    0    0  1.000000\n",
       "ADV     1     1    2     0    0     4    2     0  138    8   11    0  0.826347\n",
       "ADP     0     0    4     0    0     1    1     0    3  415    0    0  0.978774\n",
       "ADJ     4    10    2     0    0    45    6     0    7    1  217    0  0.743151\n",
       ".       0     0    0     0    0     0    0     0    0    0    0  566  1.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(test_data_df[\"actual_tag\"],test_data_df[\"only_transition_tag_pred\"]))\n",
    "print(\"Average Accuracy = %f\" %(accuracy_score(test_data_df[\"actual_tag\"],test_data_df[\"only_transition_tag_pred\"])))\n",
    "viterbi_only_trans_pred_result = pd.DataFrame(confusion_matrix(test_data_df[\"actual_tag\"],test_data_df[\"only_transition_tag_pred\"],labels=database[\"unique_tags\"]),\n",
    "                                           columns = database[\"unique_tags\"],index=database[\"unique_tags\"]).assign(accuracy = [ test_data_df[(test_data_df[\"actual_tag\"] == tag) & (test_data_df[\"only_transition_tag_pred\"] == tag)].shape[0] / test_data_df[test_data_df[\"actual_tag\"] == tag].shape[0] for tag in database[\"unique_tags\"]])\n",
    "viterbi_only_trans_pred_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbBFCmNDt9cu"
   },
   "source": [
    "The net accuracy has increased from 90.5% to 93.3%. (Vs vanilla)\n",
    "\n",
    "The net accuracy for modified vanilla with (strategy = \"backoff\") = 93.6%\n",
    "\n",
    "So both strategies give approximately same accuracy results\n",
    "\n",
    "Let look at the words predicted as X, were they present in vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T09:41:00.130780Z",
     "start_time": "2018-10-07T09:41:00.096754Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PqKuIDVzt9cv",
    "outputId": "a5b40aaf-be09-44ce-9be8-6f082d068ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words wrongly predicted, which are present in vocab =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Words wrongly predicted, which are present in vocab =\")\n",
    "list(filter(lambda x : x in database[\"vocab\"], np.unique(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"only_transition_tag_pred\"] == \"X\")][\"word\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KH_jsqWKt9cy"
   },
   "source": [
    "So all words predicted as \"X\", were not present in the Vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T09:41:05.608160Z",
     "start_time": "2018-10-07T09:41:05.590146Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "47OqQweHt9cz",
    "outputId": "c6eb345b-ede7-44e3-9905-1c14e6882d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing in vocab and wrongly predicted as \"X\" = 26\n",
      "Words which are wrongly predicted as \"X\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Bolivia', 'Braun', 'Earle', 'Green', 'Landrieu',\n",
       "       'abortion-related', 'amazingly', 'burned', 'clamped', 'construed',\n",
       "       'cultivated', 'delayed', 'disapproved', 'emasculate', 'faded',\n",
       "       'flirted', 'foam', 'hotels', 'inserted', 'non-biodegradable',\n",
       "       'outlawed', 'preclinical', 'pushes', 'shown', 'tailor-made',\n",
       "       'wrestling'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words missing in vocab and wrongly predicted as \\\"X\\\" = %d\" %(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"only_transition_tag_pred\"] == \"X\")].shape[0]))\n",
    "print(\"Words which are wrongly predicted as \\\"X\\\"\")\n",
    "np.unique(test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"only_transition_tag_pred\"] == \"X\")][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSQo7vCyt9c3"
   },
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T09:42:39.612729Z",
     "start_time": "2018-10-07T09:42:39.440554Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "agMNNG3Dt9c4",
    "outputId": "1ed9b105-35f7-428e-92f4-727070503643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words not present in vocab which were wrongly predicted as \"X\" = 297\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>Unknown_correctly_pred</th>\n",
       "      <th>Unknown_wrongly_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vanilla Viterbi</th>\n",
       "      <td>0.905014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modified Viterbi(Regex+Lexical)</th>\n",
       "      <td>0.936112</td>\n",
       "      <td>143.0</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modified Viterbi(only transition prob)</th>\n",
       "      <td>0.933150</td>\n",
       "      <td>138.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Acc  Unknown_correctly_pred  \\\n",
       "Vanilla Viterbi                         0.905014                     NaN   \n",
       "Modified Viterbi(Regex+Lexical)         0.936112                   143.0   \n",
       "Modified Viterbi(only transition prob)  0.933150                   138.0   \n",
       "\n",
       "                                        Unknown_wrongly_pred  \n",
       "Vanilla Viterbi                                          NaN  \n",
       "Modified Viterbi(Regex+Lexical)                        154.0  \n",
       "Modified Viterbi(only transition prob)                 159.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_acc = accuracy_score(test_data_df[\"actual_tag\"],test_data_df[\"vanilla_pred_tag\"])\n",
    "backoff_acc = accuracy_score(test_data_df[\"actual_tag\"],test_data_df[\"backoff_tag_pred\"])\n",
    "trans_acc = accuracy_score(test_data_df[\"actual_tag\"],test_data_df[\"only_transition_tag_pred\"])\n",
    "\n",
    "a = test_data_df[(test_data_df[\"actual_tag\"] != \"X\") & (test_data_df[\"vanilla_pred_tag\"] == \"X\")]\n",
    "backoff_unknown_corrected = a[a[\"actual_tag\"] == a[\"backoff_tag_pred\"]].shape[0]\n",
    "trans_unknown_corrected = a[a[\"actual_tag\"] == a[\"only_transition_tag_pred\"]].shape[0]\n",
    "backoff_unknown_wrongly_pred = a[a[\"actual_tag\"] != a[\"backoff_tag_pred\"]].shape[0]\n",
    "trans_unknown_wrongly_pred = a[a[\"actual_tag\"] != a[\"only_transition_tag_pred\"]].shape[0]\n",
    "\n",
    "print(\"Total number of words not present in vocab which were wrongly predicted as \\\"X\\\" = %d\" %(a.shape[0]))\n",
    "\n",
    "pd.DataFrame({\"Acc\" : [vanilla_acc,backoff_acc,trans_acc],\n",
    "              \"Unknown_correctly_pred\" : [np.nan,backoff_unknown_corrected,trans_unknown_corrected],\n",
    "              \"Unknown_wrongly_pred\" : [np.nan,backoff_unknown_wrongly_pred,trans_unknown_wrongly_pred]},index=[\"Vanilla Viterbi\",\"Modified Viterbi(Regex+Lexical)\",\"Modified Viterbi(only transition prob)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBypxUJLt9c6"
   },
   "source": [
    "Clearly, Regex+Lexical based Modified viterbi seems to be doing the job better because\n",
    "1. Number of unknowns predicted correctly are more.\n",
    "2. Number of unknowns which are wrongly predicted as are also low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZ33FVEPt9c7"
   },
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T09:44:37.018686Z",
     "start_time": "2018-10-07T09:44:37.003673Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FuQ5kRYIt9c8",
    "outputId": "2f07ab26-6392-4bb3-b372-65f094c2e6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct by backoff strategy\n",
      "Number of correction made = 147/449 ~ 0.327394\n",
      "\n",
      "Words which were corrected by backoff:\n",
      "['11.95' '115' '12.52' '126,000' '13.90' '14.75' '141.9' '143.08' '143.93'\n",
      " '149.9' '150.00' '1997' '221.4' '3.19' '306' '472' '51.6' '550,000' '609'\n",
      " '618.1' '692' '77.6' '8.75' '858,000' '879' '88.32' '89.7' '94.8' '960'\n",
      " '99.1' 'ACCOUNT' 'Aerospace' 'Ala' 'Anderson' 'Anku' 'Aquino'\n",
      " 'Birmingham' 'Bolivia' 'Braun' 'Brussels' 'Californian' 'Cataracts'\n",
      " 'Chilver' 'Clays' 'Connections' 'Corazon' 'CoreStates' 'DISCOUNT'\n",
      " 'DeFazio' 'Earle' 'GROWTH' 'Gardner' 'Green' 'Hubbard' 'Ilminster'\n",
      " 'Indexing' 'Iran-Contra' 'Kelli' 'Ketchum' 'Kirkpatrick' 'Landrieu'\n",
      " 'Louisiana-Pacific' 'Lucille' 'Malta' 'Mead' 'Metal' 'Milan' 'Mont'\n",
      " 'Moon' 'Municipal' 'N.Y' 'NCNB' 'NEW' 'Nylev' 'Old-House' 'Palmer'\n",
      " 'Paper' 'Paris' 'Peck' 'Philippine' 'Preston' 'R.D.' 'Researchers' 'S.A'\n",
      " 'Sandoz' 'Silverman' 'Somerset' 'Spending' 'Spiro' 'Sumitomo' 'Tiny'\n",
      " 'Tip' 'Tots' 'Trans' 'Video' 'Vos' 'Wick' 'Z.' 'accountants' 'adults'\n",
      " 'argues' 'assaults' 'bombers' 'broadened' 'clouding' 'crying' 'curbed'\n",
      " 'defeats' 'disapproved' 'exits' 'eye' 'filled' 'fluctuations'\n",
      " 'forest-product' 'fuels' 'genie' 'hotels' 'ignored' 'inserted'\n",
      " 'intervention' 'len' 'masters' 'maturing' 'more' 'movies' 'properties'\n",
      " 'prosecutor' 'pushes' 'reclaimed' 'refer' 'removing' 'report' 'reports'\n",
      " 'rounds' 'sagged' 'select' 'shop' 'sight' 'soaring' 'stemmed' 'tempted'\n",
      " 'viewpoints']\n"
     ]
    }
   ],
   "source": [
    "print(\"correct by backoff strategy\")\n",
    "a = test_data_df[test_data_df[\"actual_tag\"] != test_data_df[\"vanilla_pred_tag\"]]\n",
    "print(\"Number of correction made = %d/%d ~ %f\" %(a[a[\"actual_tag\"] == a[\"backoff_tag_pred\"] ].shape[0],\n",
    "                                                 a.shape[0],\n",
    "                                                 a[a[\"actual_tag\"] == a[\"backoff_tag_pred\"] ].shape[0]/a.shape[0]))\n",
    "print(\"\\nWords which were corrected by backoff:\")\n",
    "print(np.unique(a[a[\"actual_tag\"] == a[\"backoff_tag_pred\"] ][\"word\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T09:44:42.184610Z",
     "start_time": "2018-10-07T09:44:42.167594Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dEWHFfBut9c-",
    "outputId": "7054bc0b-618b-4a3e-e05e-946e7455d399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct by using only transition probability for unknown words\n",
      "Number of correction made = 142/449 ~ 0.316258\n",
      "\n",
      "Words which were corrected by using only transition prob:\n",
      "['ACCOUNT' 'Aerospace' 'Ala' 'Anderson' 'Anku' 'Aquino' 'Birmingham'\n",
      " 'Brussels' 'Californian' 'Cataracts' 'Chilver' 'Clays' 'Connections'\n",
      " 'Corazon' 'CoreStates' 'DISCOUNT' 'DeFazio' 'GROWTH' 'Gardner' 'Hubbard'\n",
      " 'Indexing' 'Iran-Contra' 'Ketchum' 'Kirkpatrick' 'Left'\n",
      " 'Louisiana-Pacific' 'Lucille' 'Mead' 'Metal' 'Milan' 'Mont' 'Municipal'\n",
      " 'N.Y' 'NCNB' 'NEW' 'Nylev' 'Old-House' 'Paper' 'Paris' 'Peck'\n",
      " 'Philippine' 'Preston' 'R.D.' 'Researchers' 'S.A' 'Sandoz' 'Silverman'\n",
      " 'Sit' 'Somerset' 'Spending' 'Spiro' 'Sumitomo' 'Tiny' 'Tip' 'Tots'\n",
      " 'Video' 'Vos' 'Wick' 'Z.' 'accountants' 'adults' 'aftereffects' 'argues'\n",
      " 'attendance' 'barometer' 'behemoth' 'bombers' 'bricks' 'broadened'\n",
      " 'chest' 'cleanup' 'clouding' 'criteria' 'curbed' 'defeats' 'diagnosed'\n",
      " 'diagram' 'directorship' 'discretion' 'display' 'down' 'dust-up'\n",
      " 'ethanol' 'executive' 'executive-office' 'exits' 'eye' 'facade' 'fleet'\n",
      " 'fluctuations' 'forest-product' 'fuels' 'genie' 'grandstander'\n",
      " 'heebie-jeebies' 'heirs' 'housewife' 'ignored' 'illustrates' 'incisions'\n",
      " 'information-services' 'initiate' 'inkling' 'insanity'\n",
      " 'insurance-company' 'intervention' 'intoxication' 'investor-relations'\n",
      " 'jolts' 'len' 'leveraging' 'luxury' 'manpower' 'marketing-communications'\n",
      " 'movies' 'outlawing' 'owning' 'polyester' 'polyproplene' 'portrayal'\n",
      " 'projector' 'properties' 'prosecutor' 'purhasing' 'relevance' 'removing'\n",
      " 'reports' 'ritual' 'rounds' 'safeguarding' 'select' 'shipboard' 'smiles'\n",
      " 'speculate' 'tempted' 'usurp' 'viewpoints']\n"
     ]
    }
   ],
   "source": [
    "print(\"correct by using only transition probability for unknown words\")\n",
    "a = test_data_df[test_data_df[\"actual_tag\"] != test_data_df[\"vanilla_pred_tag\"]]\n",
    "print(\"Number of correction made = %d/%d ~ %f\" %(a[a[\"actual_tag\"] == a[\"only_transition_tag_pred\"] ].shape[0],\n",
    "                                                 a.shape[0],\n",
    "                                                 a[a[\"actual_tag\"] == a[\"only_transition_tag_pred\"] ].shape[0]/a.shape[0]))\n",
    "print(\"\\nWords which were corrected by using only transition prob:\")\n",
    "print(np.unique(a[a[\"actual_tag\"] == a[\"only_transition_tag_pred\"] ][\"word\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fuy5mj_-t9dF"
   },
   "source": [
    "Number of corrections made\n",
    "1. By using Regex+Lexical Modified Viterbi the number of corrections = 147/449\n",
    "2. By using transition probabilities for unknown the number of corrections = 142/449\n",
    "\n",
    "Almost same performance. However, from earlier analysis we could see that, unknown words which were predicted as \"X\" by Vanilla Viterbi, were correctly predicted by Regex+Lexical better than only transition probability based viterbi heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxv_Neg6t9dH"
   },
   "source": [
    "#### Working on sample sentences\n",
    "1. We will use the vanilla viterbi to tag the sentences, and check how many words remain untagged\n",
    "2. We will use the regex+lexical tagger to tag sentences, and see how many words remain untagger\n",
    "\n",
    "*Note*\n",
    "We will use a regular expression, to seperate the punctuation signs, like \".\",\",\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:20:25.737013Z",
     "start_time": "2018-10-07T10:20:25.730007Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zWGJItvzt9dI"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "sample_sentences = list()\n",
    "with open(\"sample_sentences.txt\") as f:\n",
    "    for x in f.readlines():\n",
    "        sample_sentences.append(x)\n",
    "#sample_sentences\n",
    "sample_sentences_2 = list()\n",
    "for sent in sample_sentences:\n",
    "    temp_list = list()\n",
    "    for x in re.findall(\"([A-z0-9/'/-]*)([\\.,])*\",sent):\n",
    "        if x[0] != \"\":\n",
    "            temp_list.append(x[0])\n",
    "        if x[1] != \"\":\n",
    "            temp_list.append(x[1])\n",
    "    sample_sentences_2.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:20:38.720443Z",
     "start_time": "2018-10-07T10:20:38.714452Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rKKCsbNAt9dL",
    "outputId": "dc69719c-b37c-45c6-e750-9faec322224b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Android',\n",
       " 'is',\n",
       " 'a',\n",
       " 'mobile',\n",
       " 'operating',\n",
       " 'system',\n",
       " 'developed',\n",
       " 'by',\n",
       " 'Google',\n",
       " '.']"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXhFsVg1t9dP"
   },
   "source": [
    "Words have been collected properly, lets use **VANILLA VITERBI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:28:51.554221Z",
     "start_time": "2018-10-07T10:27:43.491203Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lOn2sY0jt9dP",
    "outputId": "b5de4b55-f70b-47f1-c162-6ac32a75382d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sample sentence 0\n",
      "Working on sample sentence 1\n",
      "Working on sample sentence 2\n",
      "Working on sample sentence 3\n",
      "Working on sample sentence 4\n",
      "Working on sample sentence 5\n",
      "Working on sample sentence 6\n",
      "Working on sample sentence 7\n",
      "Working on sample sentence 8\n",
      "Working on sample sentence 9\n",
      "Working on sample sentence 10\n"
     ]
    }
   ],
   "source": [
    "sample_sentence_df = pd.DataFrame({\"word\" : [word for sent in sample_sentences_2 for word in sent]})\n",
    "predicted_tag = list()\n",
    "for index,sent in enumerate(sample_sentences_2):\n",
    "    print(\"Working on sample sentence %d\" %(index))\n",
    "    pred_tup = vanilla_viterbi_pos_tagger(sent,db=database)\n",
    "    for x in pred_tup:\n",
    "        predicted_tag.append(x[1])\n",
    "sample_sentence_df = sample_sentence_df.assign(vanilla_viterbi_pos_tag = predicted_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:30:26.391046Z",
     "start_time": "2018-10-07T10:30:26.374054Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HifvIff8t9dT",
    "outputId": "3ccd3390-e4f1-465c-8832-198709d009b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words predicted as X = 36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vanilla_viterbi_pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Android</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mobile</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>operating</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word vanilla_viterbi_pos_tag\n",
       "0    Android                       X\n",
       "1         is                    VERB\n",
       "2          a                     DET\n",
       "3     mobile                     ADJ\n",
       "4  operating                    NOUN"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words predicted as X = %d\" %(sample_sentence_df[sample_sentence_df[\"vanilla_viterbi_pos_tag\"] == \"X\"].shape[0]))\n",
    "sample_sentence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lI71pNEIt9db"
   },
   "source": [
    "**Lets check the words predicted as X, whether they are present in library?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:30:34.654711Z",
     "start_time": "2018-10-07T10:30:34.620687Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8SFozJxqt9dc",
    "outputId": "0066c266-d114-44a8-fc51-150a7720e9a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x : x in database[\"vocab\"],np.unique(sample_sentence_df[sample_sentence_df[\"vanilla_viterbi_pos_tag\"] == \"X\"][\"word\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xa2sOsnNt9de"
   },
   "source": [
    "**So there are 36 words predicted as \"X\" and all of which are not present in VOCAB\" by VANILLA VITERBI Algo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FtkFwQst9dh"
   },
   "source": [
    "**USING MODIFIED VITERBI with Regex+Lexical based backoff technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:36:31.033021Z",
     "start_time": "2018-10-07T10:34:51.095701Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3PYrfwJAt9di",
    "outputId": "2886a98d-27c2-402a-9782-3316c7e9bb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sample sentence 0\n",
      "Working on sample sentence 1\n",
      "Working on sample sentence 2\n",
      "Working on sample sentence 3\n",
      "Working on sample sentence 4\n",
      "Working on sample sentence 5\n",
      "Working on sample sentence 6\n",
      "Working on sample sentence 7\n",
      "Working on sample sentence 8\n",
      "Working on sample sentence 9\n",
      "Working on sample sentence 10\n"
     ]
    }
   ],
   "source": [
    "predicted_tag = list()\n",
    "for index,sent in enumerate(sample_sentences_2):\n",
    "    print(\"Working on sample sentence %d\" %(index))\n",
    "    pred_tup = modified_vanilla_viterbi_pos_tagger(sent,db=database,strategy=\"backoff\")\n",
    "    for x in pred_tup:\n",
    "        predicted_tag.append(x[1])\n",
    "sample_sentence_df = sample_sentence_df.assign(modified_viterbi_backoff_pos_tag = predicted_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:36:46.168262Z",
     "start_time": "2018-10-07T10:36:46.148248Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZFfl8v3dt9dm",
    "outputId": "a3886a3c-1d0d-497f-fa9f-c47f8c860144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words predicted as X = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vanilla_viterbi_pos_tag</th>\n",
       "      <th>modified_viterbi_backoff_pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Android</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mobile</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>operating</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word vanilla_viterbi_pos_tag modified_viterbi_backoff_pos_tag\n",
       "0    Android                       X                             NOUN\n",
       "1         is                    VERB                             VERB\n",
       "2          a                     DET                              DET\n",
       "3     mobile                     ADJ                              ADJ\n",
       "4  operating                    NOUN                             NOUN"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words predicted as X = %d\" %(sample_sentence_df[sample_sentence_df[\"modified_viterbi_backoff_pos_tag\"] == \"X\"].shape[0]))\n",
    "sample_sentence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VN6C9s5Lt9dp"
   },
   "source": [
    "**Lets check the words predicted as X, whether they are present in library?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:41:26.727880Z",
     "start_time": "2018-10-07T10:41:26.707866Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aYwqodrYt9dp",
    "outputId": "0a370cdb-9da7-4d7f-9a30-c6bed6559eb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x : x in database[\"vocab\"],np.unique(sample_sentence_df[sample_sentence_df[\"modified_viterbi_backoff_pos_tag\"] == \"X\"][\"word\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjmXtg2Dt9dr"
   },
   "source": [
    "1. There are 36 words predicted as \"X\" and all of which are not present in \"VOCAB\" by VANILLA VITERBI Algo\n",
    "2. There are  8 words predicted as \"X\" and all of which are not present in \"VOCAB\" by MODIFIED VITERBI with Regex+Lexical Strategy Algo\n",
    "\n",
    "\n",
    "**USING MODIFIED VITERBI with only transition probability for unknowns technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:42:32.951128Z",
     "start_time": "2018-10-07T10:41:34.168339Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9-V6mjr_t9ds",
    "outputId": "fca7109a-ad51-43d7-acc4-ad11ccac4c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sample sentence 0\n",
      "Working on sample sentence 1\n",
      "Working on sample sentence 2\n",
      "Working on sample sentence 3\n",
      "Working on sample sentence 4\n",
      "Working on sample sentence 5\n",
      "Working on sample sentence 6\n",
      "Working on sample sentence 7\n",
      "Working on sample sentence 8\n",
      "Working on sample sentence 9\n",
      "Working on sample sentence 10\n"
     ]
    }
   ],
   "source": [
    "predicted_tag = list()\n",
    "for index,sent in enumerate(sample_sentences_2):\n",
    "    print(\"Working on sample sentence %d\" %(index))\n",
    "    pred_tup = modified_vanilla_viterbi_pos_tagger(sent,db=database,strategy=\"unknown_use_transition_only\")\n",
    "    for x in pred_tup:\n",
    "        predicted_tag.append(x[1])\n",
    "sample_sentence_df = sample_sentence_df.assign(modified_viterbi_only_transition_pos_tag = predicted_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:43:02.766727Z",
     "start_time": "2018-10-07T10:43:02.746713Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MoAoD499t9du",
    "outputId": "a97155ef-c472-49db-d3c2-ae1fd9a434cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words predicted as X = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vanilla_viterbi_pos_tag</th>\n",
       "      <th>modified_viterbi_backoff_pos_tag</th>\n",
       "      <th>modified_viterbi_only_transition_pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Android</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mobile</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>operating</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word vanilla_viterbi_pos_tag modified_viterbi_backoff_pos_tag  \\\n",
       "0    Android                       X                             NOUN   \n",
       "1         is                    VERB                             VERB   \n",
       "2          a                     DET                              DET   \n",
       "3     mobile                     ADJ                              ADJ   \n",
       "4  operating                    NOUN                             NOUN   \n",
       "\n",
       "  modified_viterbi_only_transition_pos_tag  \n",
       "0                                     NOUN  \n",
       "1                                     VERB  \n",
       "2                                      DET  \n",
       "3                                      ADJ  \n",
       "4                                     NOUN  "
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words predicted as X = %d\" %(sample_sentence_df[sample_sentence_df[\"modified_viterbi_only_transition_pos_tag\"] == \"X\"].shape[0]))\n",
    "sample_sentence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkcwpuvbt9dw"
   },
   "source": [
    "**Lets check the words predicted as X, whether they are present in library?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:43:14.402418Z",
     "start_time": "2018-10-07T10:43:14.390409Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kYtoSe-Ht9dx",
    "outputId": "d9cc6975-eceb-44e7-a997-363714c0f2e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x : x in database[\"vocab\"],np.unique(sample_sentence_df[sample_sentence_df[\"modified_viterbi_only_transition_pos_tag\"] == \"X\"][\"word\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI1Y5Qc2t9d0"
   },
   "source": [
    "1. The number of unknown words as per vanilla viterbi = 36\n",
    "2. The number of words tagged as unknown by modified viterbi (Regex+Lexical Tagger) = 8\n",
    "3. The number of words tagged as unknown by modified viterbi (using on transition prob for unknown) = 2\n",
    "\n",
    "To check the prediction results, we should compare the results of (Regex+Lexical) Vs (Only Transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T10:47:35.167864Z",
     "start_time": "2018-10-07T10:47:35.146851Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dul4hpLIt9d2",
    "outputId": "89ab823b-6328-43cd-bd92-7e7028bb06a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vanilla_viterbi_pos_tag</th>\n",
       "      <th>modified_viterbi_backoff_pos_tag</th>\n",
       "      <th>modified_viterbi_only_transition_pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Google</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>worldwide</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>smartphones</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2011</td>\n",
       "      <td>X</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2013</td>\n",
       "      <td>X</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2015</td>\n",
       "      <td>X</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Google</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Twitter's</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>online</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>messages</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>tweets</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>domineering</td>\n",
       "      <td>X</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>personality</td>\n",
       "      <td>X</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2018</td>\n",
       "      <td>X</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>21st</td>\n",
       "      <td>X</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>tournament</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>arriving</td>\n",
       "      <td>X</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>invited</td>\n",
       "      <td>X</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>ICESAT-2</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word vanilla_viterbi_pos_tag modified_viterbi_backoff_pos_tag  \\\n",
       "8         Google                       X                             NOUN   \n",
       "16     worldwide                       X                                X   \n",
       "18   smartphones                       X                                X   \n",
       "20          2011                       X                              NUM   \n",
       "25          2013                       X                              NUM   \n",
       "34          2015                       X                              NUM   \n",
       "37        Google                       X                             NOUN   \n",
       "40     Twitter's                       X                                X   \n",
       "46        online                       X                                X   \n",
       "59      messages                       X                             NOUN   \n",
       "62        tweets                       X                                X   \n",
       "72   domineering                       X                             VERB   \n",
       "77   personality                       X                              ADJ   \n",
       "80          2018                       X                              NUM   \n",
       "86          21st                       X                              NUM   \n",
       "94    tournament                       X                                X   \n",
       "162     arriving                       X                             VERB   \n",
       "168      invited                       X                             VERB   \n",
       "177     ICESAT-2                       X                                X   \n",
       "\n",
       "    modified_viterbi_only_transition_pos_tag  \n",
       "8                                        DET  \n",
       "16                                      NOUN  \n",
       "18                                       DET  \n",
       "20                                       DET  \n",
       "25                                       DET  \n",
       "34                                       DET  \n",
       "37                                         X  \n",
       "40                                      VERB  \n",
       "46                                      NOUN  \n",
       "59                                       DET  \n",
       "62                                       DET  \n",
       "72                                      NOUN  \n",
       "77                                      NOUN  \n",
       "80                                      NOUN  \n",
       "86                                      NOUN  \n",
       "94                                      NOUN  \n",
       "162                                     NOUN  \n",
       "168                                     NOUN  \n",
       "177                                      DET  "
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence_df[sample_sentence_df[\"modified_viterbi_backoff_pos_tag\"] != sample_sentence_df[\"modified_viterbi_only_transition_pos_tag\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C_iKRCfgt9d4"
   },
   "source": [
    "### Following conclusions can be made\n",
    "\n",
    "1. Using only transition probability for unknown words, may provide better metrics, however, the result quality is not good.\n",
    "2. Prediction metrics for Regex+Lexical may be slighly less, however, the prediction quality is definitely much better for Regex+Lexical based Viterbi model. Also, Regex+Lexical Viterbi backoff technique provides scope to further improve the results, by using better regex techniques also."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Anugraha_Sinha_Assignment_Syntactic_Analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
